{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1a8e1df",
   "metadata": {},
   "source": [
    "# ESIEE Paris — Data Engineering I — Assignment 2\n",
    "> Author : Badr TAJINI\n",
    "\n",
    "**Academic year:** 2025–2026  \n",
    "**Program:** Data & Applications - Engineering - (FD)   \n",
    "**Course:** Data Engineering I  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0eaf8d2f-3d54-4e91-9b23-efab19daea7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/11/05 16:35:34 WARN Utils: Your hostname, OrdideJustine, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/11/05 16:35:34 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/05 16:35:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, functions as F, types as T\n",
    "spark = SparkSession.builder.appName(\"de1-lab2-assignment\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a13f5da",
   "metadata": {},
   "source": [
    "## Data inputs\n",
    "Define your input paths. Use small CSV/JSON/Parquet files so the notebook runs locally. If your dataset requires credentials, create a **sample subset** and commit only that.\n",
    "\n",
    "**Paths to set:**\n",
    "- `SOURCE_A_PATH` (fact‑like dataset)\n",
    "- `SOURCE_B_PATH` (dimension‑like dataset)\n",
    "- `OUTPUT_BASE` (directory for Parquet output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6377a4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read carefully the helper to review what is missing here\n",
    "\n",
    "SOURCE_A_PATH = 'data/events.csv'\n",
    "SOURCE_B_PATH = 'data/user.csv'\n",
    "OUTPUT_BASE   = 'outputs'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c5ba49",
   "metadata": {},
   "source": [
    "## Pipeline API (implementations required)\n",
    "Implement the following functions. Keep signatures stable. Use explicit schemas when possible. Log counts at each stage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b712e7fb-929f-4782-9265-89f7d379da6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "def ingest(spark: SparkSession, path_a: str, path_b: str) -> Tuple[DataFrame, DataFrame]:\n",
    "    \"\"\"Load SOURCE_A and SOURCE_B. Apply explicit schemas where possible.\n",
    "    Return two DataFrames with uniform column naming.\n",
    "    \"\"\"\n",
    "    events_schema = T.StructType([\n",
    "        T.StructField(\"event_time\", T.TimestampType(), True),\n",
    "        T.StructField(\"event_type\", T.StringType(), True),\n",
    "        T.StructField(\"session_id\", T.StringType(), True),\n",
    "        T.StructField(\"product_id\", T.StringType(), True),\n",
    "        T.StructField(\"price\", T.DoubleType(), True)\n",
    "    ])\n",
    "    \n",
    "    df_events = spark.read.csv(path_a, header=True, schema=events_schema)\n",
    "    print(\"Events count:\", df_events.count())\n",
    "    \n",
    "    # Explicit schema for users\n",
    "    users_schema = T.StructType([\n",
    "        T.StructField(\"user_id\", T.StringType(), True),\n",
    "        T.StructField(\"first_name\", T.StringType(), True),\n",
    "        T.StructField(\"last_name\", T.StringType(), True),\n",
    "        T.StructField(\"birthdate\", T.DateType(), True)\n",
    "    ])\n",
    "    \n",
    "    df_users = spark.read.csv(path_b, header=True, schema=users_schema)\n",
    "    print(\"Users count:\", df_users.count())\n",
    "    \n",
    "    return df_events, df_users\n",
    "\n",
    "\n",
    "def transform(df_events: DataFrame, df_users: DataFrame) -> DataFrame:\n",
    "    \"\"\"Clean, deduplicate, and normalize. Add parsed timestamps.\n",
    "    Drop obvious null records. Prepare keys for join.\n",
    "    \"\"\"\n",
    "    df_events_clean = (\n",
    "        df_events\n",
    "        .dropna(subset=[\"event_time\", \"event_type\", \"session_id\"])\n",
    "        .dropDuplicates([\"event_time\", \"session_id\", \"product_id\"])\n",
    "    )\n",
    "    \n",
    "    # Convert timestamps to UTC if needed\n",
    "    df_events_clean = df_events_clean.withColumn(\"event_time\", F.to_utc_timestamp(\"event_time\", \"UTC\"))\n",
    "    \n",
    "    print(\"Cleaned events count:\", df_events_clean.count())\n",
    "    return df_events_clean\n",
    "\n",
    "\n",
    "def join_and_aggregate(df_events: DataFrame, df_users: DataFrame) -> DataFrame:\n",
    "    \"\"\"Join with dim table. Handle potential skew (hint: salting or AQE).\n",
    "    Compute business aggregates with window or groupBy.\n",
    "    \"\"\"\n",
    "    # Example join: left join events with users on user_id\n",
    "    # Here we assume events somehow have user_id (or you can skip join for now)\n",
    "    # If events don't have user_id, this join is skipped\n",
    "    if \"user_id\" in df_events.columns:\n",
    "        df_joined = df_events.join(df_users, on=\"user_id\", how=\"left\")\n",
    "    else:\n",
    "        df_joined = df_events\n",
    "    \n",
    "    # Compute age_on_event if birthdate exists\n",
    "    if \"birthdate\" in df_users.columns and \"user_id\" in df_joined.columns:\n",
    "        df_joined = df_joined.withColumn(\n",
    "            \"age_on_event\",\n",
    "            F.floor(F.months_between(F.col(\"event_time\"), F.col(\"birthdate\")) / 12)\n",
    "        )\n",
    "    \n",
    "    # Example aggregation: count events per day\n",
    "    df_agg = (\n",
    "        df_joined\n",
    "        .withColumn(\"event_date\", F.to_date(\"event_time\"))\n",
    "        .groupBy(\"event_date\")\n",
    "        .agg(F.count(\"*\").alias(\"events_count\"))\n",
    "    )\n",
    "    \n",
    "    return df_agg\n",
    "\n",
    "\n",
    "def write_out(df: DataFrame, base: str, partitions: list[str]) -> None:\n",
    "    \"\"\"Write Parquet, overwrite mode, partitioned by `partitions`.\n",
    "    Optimize small files if needed (coalesce).\n",
    "    \"\"\"\n",
    "    df.write.mode(\"overwrite\").partitionBy(partitions).parquet(base)\n",
    "    print(f\"Data written to {base} partitioned by {partitions}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bdc908",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "1. **Ingest**: read `SOURCE_A_PATH`, `SOURCE_B_PATH`. Provide explicit schemas. Count rows and malformed records.\n",
    "2. **Transform**: standardize column names, cast types, parse timestamps into UTC, deduplicate using keys.\n",
    "3. **Join + Aggregate**: explain your join strategy. Mitigate skew. Produce a tidy table with daily metrics.\n",
    "4. **Store**: write partitioned Parquet to `OUTPUT_BASE`, e.g., partition by `date` and one categorical column.\n",
    "5. **Explain plans**: capture `df.explain(mode='formatted')` for transform, join, and final write.\n",
    "6. **Quality gates**: implement three checks (row count non‑zero, null rate thresholds, referential coverage). Abort if a gate fails.\n",
    "7. **Reproducibility**: document your Spark config and any seeds. Describe how to re‑run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c57f9f17-f1f4-4781-9a1c-3e2acf62fe35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Events count: 42418541\n",
      "Users count: 3022290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned events count: 42412833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final count: 32\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (10)\n",
      "+- HashAggregate (9)\n",
      "   +- Exchange (8)\n",
      "      +- HashAggregate (7)\n",
      "         +- HashAggregate (6)\n",
      "            +- Exchange (5)\n",
      "               +- HashAggregate (4)\n",
      "                  +- Project (3)\n",
      "                     +- Filter (2)\n",
      "                        +- Scan csv  (1)\n",
      "\n",
      "\n",
      "(1) Scan csv \n",
      "Output [4]: [event_time#0, event_type#1, session_id#2, product_id#3]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/home/justine/Data_engineering/lab2_assignment/data/events.csv]\n",
      "ReadSchema: struct<event_time:timestamp,event_type:string,session_id:string,product_id:string>\n",
      "\n",
      "(2) Filter\n",
      "Input [4]: [event_time#0, event_type#1, session_id#2, product_id#3]\n",
      "Condition : atleastnnonnulls(3, event_time#0, event_type#1, session_id#2)\n",
      "\n",
      "(3) Project\n",
      "Output [3]: [event_time#0, session_id#2, product_id#3]\n",
      "Input [4]: [event_time#0, event_type#1, session_id#2, product_id#3]\n",
      "\n",
      "(4) HashAggregate\n",
      "Input [3]: [event_time#0, session_id#2, product_id#3]\n",
      "Keys [3]: [event_time#0, session_id#2, product_id#3]\n",
      "Functions: []\n",
      "Aggregate Attributes: []\n",
      "Results [3]: [event_time#0, session_id#2, product_id#3]\n",
      "\n",
      "(5) Exchange\n",
      "Input [3]: [event_time#0, session_id#2, product_id#3]\n",
      "Arguments: hashpartitioning(event_time#0, session_id#2, product_id#3, 200), ENSURE_REQUIREMENTS, [plan_id=284]\n",
      "\n",
      "(6) HashAggregate\n",
      "Input [3]: [event_time#0, session_id#2, product_id#3]\n",
      "Keys [3]: [event_time#0, session_id#2, product_id#3]\n",
      "Functions: []\n",
      "Aggregate Attributes: []\n",
      "Results [1]: [cast(to_utc_timestamp(event_time#0, UTC) as date) AS event_date#45]\n",
      "\n",
      "(7) HashAggregate\n",
      "Input [1]: [event_date#45]\n",
      "Keys [1]: [event_date#45]\n",
      "Functions [1]: [partial_count(1)]\n",
      "Aggregate Attributes [1]: [count#71L]\n",
      "Results [2]: [event_date#45, count#72L]\n",
      "\n",
      "(8) Exchange\n",
      "Input [2]: [event_date#45, count#72L]\n",
      "Arguments: hashpartitioning(event_date#45, 200), ENSURE_REQUIREMENTS, [plan_id=288]\n",
      "\n",
      "(9) HashAggregate\n",
      "Input [2]: [event_date#45, count#72L]\n",
      "Keys [1]: [event_date#45]\n",
      "Functions [1]: [count(1)]\n",
      "Aggregate Attributes [1]: [count(1)#53L]\n",
      "Results [2]: [event_date#45, count(1)#53L AS events_count#46L]\n",
      "\n",
      "(10) AdaptiveSparkPlan\n",
      "Output [2]: [event_date#45, events_count#46L]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written to outputs partitioned by ['event_date']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Orchestration\n",
    "# ----------------------------\n",
    "\n",
    "if spark is not None:\n",
    "    # Ingest\n",
    "    df_events, df_users = ingest(spark, SOURCE_A_PATH, SOURCE_B_PATH)\n",
    "    \n",
    "    # Transform\n",
    "    df_events_clean = transform(df_events, df_users)\n",
    "    \n",
    "    # Join + Aggregate\n",
    "    df_final = join_and_aggregate(df_events_clean, df_users)\n",
    "    \n",
    "    print(\"Final count:\", df_final.count())\n",
    "    df_final.explain(mode=\"formatted\")\n",
    "    \n",
    "    # Write output\n",
    "    write_out(df_final, OUTPUT_BASE, partitions=[\"event_date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78262290",
   "metadata": {},
   "source": [
    "# Assignment 2: ETL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1e1996-383c-4a4a-b3da-30edcd6e1762",
   "metadata": {},
   "source": [
    "## 1. Querying the Operational Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183ab400",
   "metadata": {},
   "source": [
    "Let's run a query to verify that the operational database has been properly restored and that we can issue a query to PostgreSQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1eaae5b8-7d4e-44c1-a8a5-4c89ace56231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['PGHOST'] = '127.0.0.1'   # adresse du serveur PostgreSQL\n",
    "os.environ['PGPORT'] = '5433'        # ou 5432 selon ton installation\n",
    "os.environ['PGUSER'] = 'esiee_reader'\n",
    "os.environ['PGPASSWORD'] = 'azerty123'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a33f61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " number_users \n",
      "--------------\n",
      "      3022290\n",
      "(1 row)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!psql \"esiee_full\" -v ON_ERROR_STOP=1 -c \"SELECT COUNT(DISTINCT user_id) AS number_users FROM retail.user;\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3851c65a-e658-4044-8878-4c3d3722463e",
   "metadata": {},
   "source": [
    "**The correct answer should be 3022290.**\n",
    "\n",
    "If running the cell above gives you the same answer, the everything should be in order.\n",
    "\n",
    "If you're getting an error, fix it before moving on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d489b61-7304-4513-8c79-89c07745a3ec",
   "metadata": {},
   "source": [
    "As a warmup exercise, write SQL queries against the operational database to answer the following questions and report the answers.\n",
    "Place both your SQL queries and answers in the following cell, replacing the placeholder texts that exist there.\n",
    "Each question needs to be answered by a _single_ SQL query (that is, it is not acceptable to run multiple SQL queries and then compute the answer yourself).\n",
    "\n",
    "1. For `session_id` `789d3699-028e-4367-b515-b82e2cb5225f`, what was the purchase price?\n",
    "2. How many products are sold by the brand \"sokolov\"?\n",
    "3. What is the average purchase price of items purchased from the brand \"febest\"?\n",
    "4. What is average number of events per user? (Report answer to two digits after the decimal point, i.e., XX.XX)\n",
    "\n",
    "**write some code here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a6bfddd-9077-4819-a842-47b0e40add2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " price  \n",
      "--------\n",
      " 100.39\n",
      "(1 row)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!psql \"esiee_full\" -v ON_ERROR_STOP=1 -c \"SELECT price FROM retail.events WHERE session_id = '789d3699-028e-4367-b515-b82e2cb5225f' AND event_type = 'purchase';\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54b82816-13c3-4e77-aae8-e1ccb8f1752f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " nb_produits \n",
      "-------------\n",
      "         354\n",
      "(1 row)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!psql \"esiee_full\" -v ON_ERROR_STOP=1 -c \"SELECT COUNT(DISTINCT p.product_id) AS nb_produits FROM retail.product p JOIN retail.events e ON p.product_id = e.product_id WHERE p.brand = 'sokolov' AND e.event_type = 'purchase';\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf68b2a3-2a86-454b-9362-fafc7599e385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " prix_moyen \n",
      "------------\n",
      "      20.39\n",
      "(1 row)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!psql \"esiee_full\" -v ON_ERROR_STOP=1 -c \"SELECT ROUND(AVG(e.price), 2) AS prix_moyen FROM retail.product p JOIN retail.events e ON p.product_id = e.product_id WHERE p.brand = 'febest' AND e.event_type = 'purchase';\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91dfa5ce-1475-4a10-8f78-ba5516fd998c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " events_par_user \n",
      "-----------------\n",
      "           14.04\n",
      "(1 row)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!psql \"esiee_full\" -v ON_ERROR_STOP=1 -c \"SELECT ROUND(AVG(ev_count), 2) AS events_par_user FROM (SELECT user_id, COUNT(*) AS ev_count FROM retail.session s JOIN retail.events e ON s.session_id = e.session_id GROUP BY user_id) t;\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b515132-07e2-44e3-abea-287b0a89af4a",
   "metadata": {},
   "source": [
    "// qcell_1b76x2 (keep this id for tracking purposes)\n",
    "\n",
    "**Q1 SQL:**\n",
    "\n",
    "SELECT price\n",
    "FROM retail.events\n",
    "WHERE session_id = '789d3699-028e-4367-b515-b82e2cb5225f'\n",
    "  AND event_type = 'purchase';\n",
    "\n",
    "\n",
    "**Q1 answer:**\n",
    "\n",
    "100.39\n",
    "\n",
    "**Q2 SQL:**\n",
    "\n",
    "SELECT COUNT(DISTINCT p.product_id) AS nb_produits\n",
    "FROM retail.product p\n",
    "JOIN retail.events e ON p.product_id = e.product_id\n",
    "WHERE p.brand = 'sokolov'\n",
    "  AND e.event_type = 'purchase';\n",
    "\n",
    "**Q2 answer:**\n",
    "\n",
    "354\n",
    "\n",
    "**Q3 SQL:**\n",
    "\n",
    "SELECT ROUND(AVG(e.price), 2) AS prix_moyen\n",
    "FROM retail.product p\n",
    "JOIN retail.events e ON p.product_id = e.product_id\n",
    "WHERE p.brand = 'febest'\n",
    "  AND e.event_type = 'purchase';\n",
    "\n",
    "**Q3 answer:**\n",
    "20.39\n",
    "\n",
    "\n",
    "**Q4 SQL:**\n",
    "\n",
    "SELECT ROUND(AVG(ev_count), 2) AS events_par_user\n",
    "FROM (\n",
    "    SELECT user_id, COUNT(*) AS ev_count\n",
    "    FROM retail.session s\n",
    "    JOIN retail.events e ON s.session_id = e.session_id\n",
    "    GROUP BY user_id\n",
    ") t;\n",
    "\n",
    "**Q4 answer:**\n",
    "\n",
    "14.04\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fecda4-fc6e-4767-af8f-c08d72eeaa02",
   "metadata": {},
   "source": [
    "## 2. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ec996e",
   "metadata": {},
   "source": [
    "The following cell contains setup to measure wall clock time and memory usage. (Don't worry about the details, just run the cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39cbf5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/justine/miniconda3/envs/de1-env/lib/python3.10/site-packages (2.2.6)\n",
      "Requirement already satisfied: pandas in /home/justine/miniconda3/envs/de1-env/lib/python3.10/site-packages (2.3.3)\n",
      "Collecting pyarrow\n",
      "  Downloading pyarrow-22.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.7-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/justine/miniconda3/envs/de1-env/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/justine/miniconda3/envs/de1-env/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/justine/miniconda3/envs/de1-env/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.60.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (112 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.9-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/justine/miniconda3/envs/de1-env/lib/python3.10/site-packages (from matplotlib) (25.0)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-12.0.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.8 kB)\n",
      "Collecting pyparsing>=3 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/justine/miniconda3/envs/de1-env/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pyarrow-22.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (47.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.6/47.6 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m  \u001b[33m0:00:25\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.10.7-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0m eta \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hDownloading scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.7/37.7 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m  \u001b[33m0:00:20\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.60.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (4.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.9-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pillow-12.0.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (7.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0m eta \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hDownloading pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
      "Installing collected packages: scipy, pyparsing, pyarrow, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9/9\u001b[0m [matplotlib]9\u001b[0m [matplotlib]\n",
      "\u001b[1A\u001b[2KSuccessfully installed contourpy-1.3.2 cycler-0.12.1 fonttools-4.60.1 kiwisolver-1.4.9 matplotlib-3.10.7 pillow-12.0.0 pyarrow-22.0.0 pyparsing-3.2.5 scipy-1.15.3\n",
      "psutil is installed.\n"
     ]
    }
   ],
   "source": [
    "!pip install -U numpy pandas pyarrow matplotlib scipy\n",
    "import sys, subprocess\n",
    "try:\n",
    "    import psutil  # noqa: F401\n",
    "except Exception:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"psutil\"])\n",
    "print(\"psutil is installed.\")\n",
    "\n",
    "\n",
    "from IPython.core.magic import register_cell_magic\n",
    "import time, os, platform\n",
    "\n",
    "# Try to import optional modules\n",
    "try:\n",
    "    import psutil\n",
    "except Exception:\n",
    "    psutil = None\n",
    "\n",
    "try:\n",
    "    import resource  # not available on Windows\n",
    "except Exception:\n",
    "    resource = None\n",
    "\n",
    "\n",
    "def _rss_bytes():\n",
    "    \"\"\"Resident Set Size in bytes (cross-platform via psutil if available).\"\"\"\n",
    "    if psutil is not None:\n",
    "        return psutil.Process(os.getpid()).memory_info().rss\n",
    "    # Fallback: unknown RSS → 0 \n",
    "    return 0\n",
    "\n",
    "\n",
    "def _peak_bytes():\n",
    "    \"\"\"\n",
    "    Best-effort peak memory in bytes.\n",
    "    - Windows: psutil peak working set (peak_wset)\n",
    "    - Linux:   resource.ru_maxrss (KB → bytes)\n",
    "    - macOS:   resource.ru_maxrss (bytes)\n",
    "    Fallback to current RSS if unavailable.\n",
    "    \"\"\"\n",
    "    sysname = platform.system()\n",
    "\n",
    "    # Windows path: use psutil peak_wset if present\n",
    "    if sysname == \"Windows\" and psutil is not None:\n",
    "        mi = psutil.Process(os.getpid()).memory_info()\n",
    "        peak = getattr(mi, \"peak_wset\", None)  # should be available on Windows\n",
    "        if peak is not None:\n",
    "            return int(peak)\n",
    "        return int(mi.rss)\n",
    "\n",
    "    # POSIX path: resource may be available\n",
    "    if resource is not None:\n",
    "        try:\n",
    "            ru = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n",
    "            # On Linux ru_maxrss is in kilobytes; on macOS/BSD it is bytes\n",
    "            if sysname == \"Linux\":\n",
    "                return int(ru) * 1024\n",
    "            else:\n",
    "                return int(ru)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Last resort\n",
    "    return _rss_bytes()\n",
    "\n",
    "\n",
    "@register_cell_magic\n",
    "def timemem(line, cell):\n",
    "    \"\"\"\n",
    "    Measure wall time and memory around the execution of this cell.\n",
    "\n",
    "        %%timemem\n",
    "        <your code>\n",
    "\n",
    "    Notes:\n",
    "    - RSS = resident memory after the cell.\n",
    "    - Peak is OS-dependent (see _peak_bytes docstring).\n",
    "    \"\"\"\n",
    "    ip = get_ipython()\n",
    "\n",
    "    rss_before  = _rss_bytes()\n",
    "    peak_before = _peak_bytes()\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    # Execute the cell body\n",
    "    result = ip.run_cell(cell)\n",
    "\n",
    "    t1 = time.perf_counter()\n",
    "    rss_after  = _rss_bytes()\n",
    "    peak_after = _peak_bytes()\n",
    "\n",
    "    wall = t1 - t0\n",
    "    rss_delta_mb  = (rss_after  - rss_before)  / (1024 * 1024)\n",
    "    peak_delta_mb = (peak_after - peak_before) / (1024 * 1024)\n",
    "\n",
    "    print(\"======================================\")\n",
    "    print(f\"Wall time: {wall:.3f} s\")\n",
    "    print(f\"RSS Δ: {rss_delta_mb:+.2f} MB\")\n",
    "    print(f\"Peak memory Δ: {peak_delta_mb:+.2f} MB (OS-dependent)\")\n",
    "    print(\"======================================\")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620ca1f2-5c25-49df-9570-f5e56782b55c",
   "metadata": {},
   "source": [
    "## 3. The \"Extract\" in ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acb0763",
   "metadata": {},
   "source": [
    "The operational database comprises the tables described in the helper.\n",
    "\n",
    "For the \"Extract\" in ETL, we're going to extract the following CSV files, each corresponding to a table in the operational database:\n",
    "\n",
    "- **user.csv**: `user_id, gender, birthdate`\n",
    "- **session.csv**: `session_id, user_id`\n",
    "- **product.csv**: `product_id, brand, category, product_name`\n",
    "- **product_name.csv**: `category, product_name, description`\n",
    "- **events.csv**: `event_time, event_type, session_id, product_id, price`\n",
    "- **category.csv**: `category, description`\n",
    "- **brand.csv**: `brand, description`\n",
    "\n",
    "From these files, we'll build a data warehouse organized in a standard star schema that has the following tables:\n",
    "\n",
    "- Dimension tables: `dim_user`, `dim_age`, `dim_brand`, `dim_category`, `dim_product`, `dim_date`, `dim_session`\n",
    "- The main fact table `fact_events` with foreign keys: `date_key, user_key, age_key, product_key, brand_key, category_key, session_key`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d94b96-231a-4db7-9c60-5ae521a38aae",
   "metadata": {},
   "source": [
    "Let's specify a \"base directory\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8fd673d3-8c30-489a-822c-27419c1e1a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to path on your local machine.\n",
    "BASE_DIR = \"/home/justine/de1-website/DE1/labs-final/lab2-assignment/csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f25b07b-0a28-499a-9817-0630640aea20",
   "metadata": {},
   "source": [
    "These are the commands that perform the \"extraction\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626f430b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!psql cs451 -v ON_ERROR_STOP=1 -c '\\copy \"retail\".\"user\"         TO '\\''{BASE_DIR}/user.csv'\\''         WITH (FORMAT csv, HEADER true)'\n",
    "!psql cs451 -v ON_ERROR_STOP=1 -c '\\copy \"retail\".\"session\"      TO '\\''{BASE_DIR}/session.csv'\\''      WITH (FORMAT csv, HEADER true)'\n",
    "!psql cs451 -v ON_ERROR_STOP=1 -c '\\copy \"retail\".\"category\"     TO '\\''{BASE_DIR}/category.csv'\\''     WITH (FORMAT csv, HEADER true)'\n",
    "!psql cs451 -v ON_ERROR_STOP=1 -c '\\copy \"retail\".\"brand\"        TO '\\''{BASE_DIR}/brand.csv'\\''        WITH (FORMAT csv, HEADER true)'\n",
    "!psql cs451 -v ON_ERROR_STOP=1 -c '\\copy \"retail\".\"product_name\" TO '\\''{BASE_DIR}/product_name.csv'\\'' WITH (FORMAT csv, HEADER true)'\n",
    "!psql cs451 -v ON_ERROR_STOP=1 -c '\\copy \"retail\".\"product\"      TO '\\''{BASE_DIR}/product.csv'\\''      WITH (FORMAT csv, HEADER true)'\n",
    "!psql cs451 -v ON_ERROR_STOP=1 -c '\\copy \"retail\".\"events\"       TO '\\''{BASE_DIR}/events.csv'\\''       WITH (FORMAT csv, HEADER true)'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9f0dde-6c17-48ad-aa4e-6e4019b106bc",
   "metadata": {},
   "source": [
    "(Note that the quote style above will _not_ work for Windows machines. Please adjust accordingly.)\n",
    "\n",
    "After the extraction, you should have 7 CSV files, each corresponding to a table in the operational database.\n",
    "\n",
    "The CSV files should be stored in `BASE_DIR`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0930cdf5-b989-4608-8a03-22c0eab6b031",
   "metadata": {},
   "source": [
    "The following code snippet should \"just work\" to initialize Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fe303ff-e323-4389-a64f-d9e730fa1ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.255.255.254:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>de1-lab2-assignment</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x763b69eb2110>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark, os, sys\n",
    "\n",
    "# Change to path on your local machine.\n",
    "os.environ[\"SPARK_HOME\"] = \"home/justine/spark-4.0.1-bin-hadoop3\"\n",
    "findspark.init()\n",
    "\n",
    "import os\n",
    "from pyspark.sql import SparkSession, functions as F, types as T\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "py = sys.executable  # the Python of this notebook (e.g., .../envs/yourenv/bin/python)\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = py\n",
    "os.environ[\"PYSPARK_PYTHON\"] = py\n",
    "\n",
    "spark = SparkSession.getActiveSession() or (\n",
    "    SparkSession.builder\n",
    "    .appName(\"A2\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.driver.memory\", \"8g\")           # or 12g+\n",
    "    .config(\"spark.sql.shuffle.partitions\",\"400\")\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    .config(\"spark.pyspark.driver.python\", py)\n",
    "    .config(\"spark.pyspark.python\", py)\n",
    "    .config(\"spark.executorEnv.PYSPARK_PYTHON\", py)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eec616b-b333-4eb6-8d7e-ab002af8f668",
   "metadata": {},
   "source": [
    "At this point, Spark should be initialized.\n",
    "\n",
    "Let's then load in CSV files into DataFrames.\n",
    "\n",
    "write some code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "49534dee-f56a-4f6b-8930-64092a0fcba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: 3022290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session: 9244421\n",
      "product: 166794\n",
      "product_name: 127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "events: 42418541\n",
      "category: 13\n",
      "brand: 3444\n",
      "======================================\n",
      "Wall time: 11.014 s\n",
      "RSS Δ: +0.12 MB\n",
      "Peak memory Δ: +0.12 MB (OS-dependent)\n",
      "======================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExecutionResult object at 712f3ac35cf0, execution_count=None error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 712f3ac354b0, raw_cell=\"# codecell_30z8le (keep this id for tracking purpo..\" store_history=False silent=False shell_futures=True cell_id=None> result=None>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%timemem\n",
    "# codecell_30z8le (keep this id for tracking purposes)\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, DoubleType\n",
    "\n",
    "USER_CSV        = f\"{BASE_DIR}/user.csv\"\n",
    "SESSION_CSV     = f\"{BASE_DIR}/session.csv\"\n",
    "PRODUCT_CSV     = f\"{BASE_DIR}/product.csv\"\n",
    "PRODUCT_NAME_CSV= f\"{BASE_DIR}/product_name.csv\"\n",
    "EVENTS_CSV      = f\"{BASE_DIR}/events.csv\"\n",
    "CATEGORY_CSV    = f\"{BASE_DIR}/category.csv\"\n",
    "BRAND_CSV       = f\"{BASE_DIR}/brand.csv\"\n",
    "\n",
    "events_schema = StructType([\n",
    "    StructField(\"event_time\", TimestampType(), True),\n",
    "    StructField(\"event_type\", StringType(), True),\n",
    "    StructField(\"session_id\", StringType(), True),\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"price\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "df_user         = spark.read.csv(USER_CSV, header=True, inferSchema=True)\n",
    "df_session      = spark.read.csv(SESSION_CSV, header=True, inferSchema=True)\n",
    "df_product      = spark.read.csv(PRODUCT_CSV, header=True, inferSchema=True)\n",
    "df_product_name = spark.read.csv(PRODUCT_NAME_CSV, header=True, inferSchema=True)\n",
    "df_events       = spark.read.csv(EVENTS_CSV, header=True, schema=events_schema)\n",
    "df_category     = spark.read.csv(CATEGORY_CSV, header=True, inferSchema=True)\n",
    "df_brand        = spark.read.csv(BRAND_CSV, header=True, inferSchema=True)\n",
    "\n",
    "# Vérifier les counts\n",
    "print(f\"user: {df_user.count()}\")\n",
    "print(f\"session: {df_session.count()}\")\n",
    "print(f\"product: {df_product.count()}\")\n",
    "print(f\"product_name: {df_product_name.count()}\")\n",
    "print(f\"events: {df_events.count()}\")\n",
    "print(f\"category: {df_category.count()}\")\n",
    "print(f\"brand: {df_brand.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4552f156-6f37-4938-8e93-1a02a38ba760",
   "metadata": {},
   "source": [
    "How do you know if you've done everything correctly?\n",
    "\n",
    "Well, issue the SQL query `select count(*) from retail.user;` to count the number of rows in the `user` table in the operational database.\n",
    "It should match the output of `df_user.count()`; same for the other tables.\n",
    "If the counts match, then you know everything is in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d064b75-be87-4fa3-8730-243fa8e88396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  count  \n",
      "---------\n",
      " 3022290\n",
      "(1 row)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!psql \"esiee_full\" -v ON_ERROR_STOP=1 -c \"SELECT COUNT(*) FROM retail.user;\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e10f63",
   "metadata": {},
   "source": [
    "## 4. Build the Dimensions Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3303f4-e9be-455b-a458-91de0ebdded7",
   "metadata": {},
   "source": [
    "### 4.1 The `user` Dimension Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f4ee83",
   "metadata": {},
   "source": [
    "Build the `dim_user` dimension table.\n",
    "This table should include `user_key`, `user_id`, `gender`, `birthdate`, and `generation`. \n",
    "\n",
    "Set `generation` to one of the following values based on the birth year: \n",
    "- \"Traditionalists\": born 1925 to 1945\n",
    "- \"Boomers\": born 1946 to 1964\n",
    "- \"GenX\": born 1965 to 1980\n",
    "- \"Millennials\": born 1981 to 2000\n",
    "- \"GenZ\": born 2001 to 2020\n",
    "\n",
    "**write some code here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "883cc05a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3022290"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================\n",
      "Wall time: 0.557 s\n",
      "RSS Δ: +0.00 MB\n",
      "Peak memory Δ: +0.00 MB (OS-dependent)\n",
      "======================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExecutionResult object at 712f3ac347f0, execution_count=None error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 712f3ac34130, raw_cell=\"# codecell_41ax14 (keep this id for tracking purpo..\" store_history=False silent=False shell_futures=True cell_id=None> result=3022290>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%timemem\n",
    "# codecell_41ax14 (keep this id for tracking purposes)\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "w = Window.orderBy(\"user_id\")\n",
    "dim_user = df_user.withColumn(\"user_key\", F.row_number().over(w))\n",
    "\n",
    "dim_user = dim_user.withColumn(\"birth_year\", F.year(\"birthdate\"))\n",
    "\n",
    "dim_user = dim_user.withColumn(\n",
    "    \"generation\",\n",
    "    F.when((F.col(\"birth_year\") >= 1925) & (F.col(\"birth_year\") <= 1945), \"Traditionalists\")\n",
    "     .when((F.col(\"birth_year\") >= 1946) & (F.col(\"birth_year\") <= 1964), \"Boomers\")\n",
    "     .when((F.col(\"birth_year\") >= 1965) & (F.col(\"birth_year\") <= 1980), \"GenX\")\n",
    "     .when((F.col(\"birth_year\") >= 1981) & (F.col(\"birth_year\") <= 2000), \"Millennials\")\n",
    "     .when((F.col(\"birth_year\") >= 2001) & (F.col(\"birth_year\") <= 2020), \"GenZ\")\n",
    "     .otherwise(\"unknown\")\n",
    ")\n",
    "\n",
    "dim_user = dim_user.select(\"user_key\", \"user_id\", \"gender\", \"birthdate\", \"generation\")\n",
    "\n",
    "\n",
    "# By the time we get to here, \"dim_user\" should hold the user dimensions table according to the specification above.\n",
    "\n",
    "dim_user.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e6e753-8515-4924-b4d7-f4a8eeb3c4f8",
   "metadata": {},
   "source": [
    "**The correct answer should be 3022290.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0eadced-3481-480a-9320-a07bbfc6d6b7",
   "metadata": {},
   "source": [
    "### 4.2 The `age` Dimension Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6758f1e8",
   "metadata": {},
   "source": [
    "Even though `birthdate` exists in `dim_user`, a separate `dim_age` is helpful because it:\n",
    "- Simplifies analysis with ready-made bands.\n",
    "- Ensures consistency across all queries.\n",
    "- Improves performance via small surrogate keys.\n",
    "- Preserves history by fixing age at event time.\n",
    "- Adds flexibility to adjust bands without changing facts.\n",
    "\n",
    "We're going to build a `dim_age` table that has 4 columns:\n",
    "- `age_key`: (INT, surrogate PK)\n",
    "- `age_band`: (STRING) following the age band rules below\n",
    "- `min_age`: (INT)\n",
    "- `max_age`: (INT)\n",
    "\n",
    "Bands:\n",
    "- \"<18\": min_age = NULL, max_age = 17\n",
    "- \"18-24\": 18, 24\n",
    "- \"25-34\": 25, 34\n",
    "- \"35-44\": 35, 44\n",
    "- \"45-54\": 45, 54\n",
    "- \"55-64\": 55, 64\n",
    "- \"65-74\": 65, 74\n",
    "- \"75-84\": 75, 84\n",
    "- \"85-94\": 85, 94\n",
    "- \"unknown\": NULL, NULL\n",
    "\n",
    "The construction of this table is a bit tricky, so we're going to show you how to do it, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ed16029",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================\n",
      "Wall time: 1.482 s\n",
      "RSS Δ: +26.00 MB\n",
      "Peak memory Δ: +20.29 MB (OS-dependent)\n",
      "======================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExecutionResult object at 712f3ac36920, execution_count=None error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 712f3ac374c0, raw_cell=\"\n",
       "# Static age bands\n",
       "age_band_rows = [\n",
       "    (\"<18\", ..\" store_history=False silent=False shell_futures=True cell_id=None> result=10>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%timemem\n",
    "\n",
    "# Static age bands\n",
    "age_band_rows = [\n",
    "    (\"<18\",   None, 17),\n",
    "    (\"18-24\", 18, 24),\n",
    "    (\"25-34\", 25, 34),\n",
    "    (\"35-44\", 35, 44),\n",
    "    (\"45-54\", 45, 54),\n",
    "    (\"55-64\", 55, 64),\n",
    "    (\"65-74\", 65, 74),\n",
    "    (\"75-84\", 75, 84),\n",
    "    (\"85-94\", 85, 94),\n",
    "    (\"unknown\", None, None),\n",
    "]\n",
    "dim_age = spark.createDataFrame(age_band_rows, [\"age_band\", \"min_age\", \"max_age\"])\n",
    "\n",
    "w_age = Window.orderBy(F.col(\"age_band\"))\n",
    "dim_age = dim_age.withColumn(\"age_key\", F.dense_rank().over(w_age))\n",
    "\n",
    "dim_age.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55857d00-560c-4beb-8d29-8626369a3110",
   "metadata": {},
   "source": [
    "**The correct answer should be 10.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6012e1ca-e6ca-46ae-a873-7f5771cd4310",
   "metadata": {},
   "source": [
    "### 4.3 The `brand`, `product`, and `category` Dimension Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe029f8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Build the following dimension tables:\n",
    "\n",
    "**`dim_brand`:**\n",
    "- `brand_key` (INT, surrogate PK)\n",
    "- `brand_code` (STRING) \n",
    "- `brand_desc` (STRING)\n",
    "\n",
    "**`dim_category`:**\n",
    "- `category_key` (INT, surrogate PK)\n",
    "- `category_code` (STRING) \n",
    "- `category_desc` (STRING)\n",
    "\n",
    "**`dim_product`:**\n",
    "- `product_key`  (INT, surrogate PK)\n",
    "- `product_id`   (STRING)\n",
    "- `product_desc` (STRING)\n",
    "- `brand_key`   (INT, FK → `dim_brand`)  \n",
    "- `category_key`(INT, FK → `dim_category`)\n",
    "\n",
    "The Learning goals of `dim_product` is to keep all products in `product`, and add details from `product_names`, then join the results with `brand` and `category` dimension tables.\n",
    "\n",
    "**write some code here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0acddfd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in dim_brand: 3444\n",
      "Number of rows in dim_category: 13\n",
      "Number of rows in dim_product: 166794\n",
      "======================================\n",
      "Wall time: 1.130 s\n",
      "RSS Δ: +0.25 MB\n",
      "Peak memory Δ: +0.25 MB (OS-dependent)\n",
      "======================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExecutionResult object at 712f3932a7a0, execution_count=None error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 712f3932a770, raw_cell=\"# codecell_43k3n9 (keep this id for tracking purpo..\" store_history=False silent=False shell_futures=True cell_id=None> result=None>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%timemem\n",
    "# codecell_43k3n9 (keep this id for tracking purposes)\n",
    "\n",
    "# 1. dim_brand\n",
    "w_brand = Window.orderBy(\"brand\")\n",
    "dim_brand = df_brand.withColumn(\"brand_key\", F.row_number().over(w_brand))\n",
    "dim_brand = dim_brand.select(\n",
    "    \"brand_key\",\n",
    "    F.col(\"brand\").alias(\"brand_code\"),\n",
    "    F.col(\"description\").alias(\"brand_desc\")\n",
    ")\n",
    "# 2. dim_category\n",
    "w_cat = Window.orderBy(\"category\")\n",
    "dim_category = df_category.withColumn(\"category_key\", F.row_number().over(w_cat))\n",
    "dim_category = dim_category.select(\n",
    "    \"category_key\",\n",
    "    F.col(\"category\").alias(\"category_code\"),\n",
    "    F.col(\"description\").alias(\"category_desc\")\n",
    ")\n",
    "\n",
    "# 3. dim_product\n",
    "df_product_full = df_product.join(\n",
    "    df_product_name,\n",
    "    on=[\"category\", \"product_name\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "df_product_full = df_product_full.join(\n",
    "    dim_brand,\n",
    "    df_product_full.brand == dim_brand.brand_code,\n",
    "    how=\"left\"\n",
    ").join(\n",
    "    dim_category,\n",
    "    df_product_full.category == dim_category.category_code,\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "w_prod = Window.orderBy(\"product_id\")\n",
    "dim_product = df_product_full.withColumn(\"product_key\", F.row_number().over(w_prod))\n",
    "dim_product = dim_product.select(\n",
    "    \"product_key\",\n",
    "    \"product_id\",\n",
    "    F.col(\"description\").alias(\"product_desc\"),\n",
    "    \"brand_key\",\n",
    "    \"category_key\"\n",
    ")\n",
    "\n",
    "# By the time we get to here, \"dim_brand\", \"dim_category\", and \"dim_product\" should hold \n",
    "# the dimension tables according to the specifications above.\n",
    "\n",
    "print(f\"Number of rows in dim_brand: {dim_brand.count()}\")\n",
    "print(f\"Number of rows in dim_category: {dim_category.count()}\")\n",
    "print(f\"Number of rows in dim_product: {dim_product.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b819a9-6b4c-4a1f-82a3-e379d1d6a2ad",
   "metadata": {},
   "source": [
    "**Correct answers:**\n",
    "\n",
    "+ Number of rows in `dim_brand`: 3444\n",
    "+ Number of rows in `dim_category`: 13\n",
    "+ Number of rows in `dim_product`: 166794"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60db6b6a-b5af-46ac-ab1c-161cf50ea75a",
   "metadata": {},
   "source": [
    "### 4.4  The `date` Dimension Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504f27d5",
   "metadata": {},
   "source": [
    "This table is expected to have one row per calendar date. \n",
    "\n",
    "**`dim_date`:**\n",
    "- `date_key`     (INT, surrogate PK; format YYYYMMDD)\n",
    "- `date`         (DATE, the actual calendar date)\n",
    "- `day`          (INT, 1–31)\n",
    "- `day_of_week`  (INT, 1=Mon … 7=Sun)\n",
    "- `day_name`     (STRING, e.g., Monday)\n",
    "- `is_weekend`   (BOOLEAN)\n",
    "- `week_of_year` (INT, 1–53, ISO week)\n",
    "- `month`        (INT, 1–12)\n",
    "- `month_name`   (STRING, e.g., January)\n",
    "- `quarter`      (INT, 1–4)\n",
    "- `year`         (INT)\n",
    "\n",
    "\n",
    "There are 2025 years, each with 365 days. Do we need to have a table that big? \n",
    "We can, but we do not have to! \n",
    "\n",
    "Instead, follow these instructions to create only as many rows as we need:\n",
    "\n",
    "1. Determine the date range (from the min and max `event_date` in `df_events`).\n",
    "2. Generate all dates in that range with `F.sequence()`.\n",
    "3. Derive attributes (`day`, `day_of_week`, ...).\n",
    "4. Create `date_key` = `year * 10000 + month * 100 + day` (i.e., YYYYMMDD).\n",
    "5. Assign `date_key` as the surrogate PK.\n",
    "\n",
    "Build the `dim_date` table conforming to the specifications above.\n",
    "\n",
    "**write some code here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "417d0554",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "======================================\n",
      "Wall time: 20.889 s\n",
      "RSS Δ: +0.00 MB\n",
      "Peak memory Δ: +0.00 MB (OS-dependent)\n",
      "======================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExecutionResult object at 712f3ac37e20, execution_count=None error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 712f3ac35420, raw_cell=\"# codecell_44qm5c (keep this id for tracking purpo..\" store_history=False silent=False shell_futures=True cell_id=None> result=None>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%timemem\n",
    "# codecell_44qm5c (keep this id for tracking purposes)\n",
    "\n",
    "min_date, max_date = df_events.select(\n",
    "    F.min(F.to_date(\"event_time\")).alias(\"min_date\"),\n",
    "    F.max(F.to_date(\"event_time\")).alias(\"max_date\")\n",
    ").first()\n",
    "\n",
    "dates_df = spark.createDataFrame([(min_date, max_date)], [\"start\", \"end\"]) \\\n",
    "    .withColumn(\"date\", F.explode(F.sequence(F.col(\"start\"), F.col(\"end\"), F.expr(\"interval 1 day\")))) \\\n",
    "    .select(\"date\")\n",
    "\n",
    "dim_date = dates_df.withColumn(\"day\", F.dayofmonth(\"date\")) \\\n",
    "    .withColumn(\"day_of_week\", F.date_format(\"date\", \"u\").cast(\"int\")) \\\n",
    "    .withColumn(\"day_name\", F.date_format(\"date\", \"EEEE\")) \\\n",
    "    .withColumn(\"is_weekend\", (F.col(\"day_of_week\") >= 6).cast(\"boolean\")) \\\n",
    "    .withColumn(\"week_of_year\", F.weekofyear(\"date\")) \\\n",
    "    .withColumn(\"month\", F.month(\"date\")) \\\n",
    "    .withColumn(\"month_name\", F.date_format(\"date\", \"MMMM\")) \\\n",
    "    .withColumn(\"quarter\", F.quarter(\"date\")) \\\n",
    "    .withColumn(\"year\", F.year(\"date\")) \\\n",
    "    .withColumn(\"date_key\", F.col(\"year\") * 10000 + F.col(\"month\") * 100 + F.col(\"day\"))\n",
    "\n",
    "dim_date = dim_date.select(\n",
    "    \"date_key\", \"date\", \"day\", \"day_of_week\", \"day_name\", \"is_weekend\",\n",
    "    \"week_of_year\", \"month\", \"month_name\", \"quarter\", \"year\"\n",
    ")\n",
    "# By the time we get to here, \"dim_date\" should hold the dates dimension table according to the specification above.\n",
    "\n",
    "print(dim_date.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6191b0ce-16fe-4359-bdc2-b23030c5e4f0",
   "metadata": {},
   "source": [
    "**The correct answer should be 32.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e838229",
   "metadata": {},
   "source": [
    "If you reach here, congratulations!\n",
    "You have created all the dimension tables!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e6ad489b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_user: 3022290\n",
      "dim_age: 10\n",
      "dim_brand: 3444\n",
      "dim_category: 13\n",
      "dim_product: 166794\n",
      "dim_date: 32\n",
      "======================================\n",
      "Wall time: 1.472 s\n",
      "RSS Δ: +0.00 MB\n",
      "Peak memory Δ: +0.00 MB (OS-dependent)\n",
      "======================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExecutionResult object at 712f3ac371f0, execution_count=None error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 712f3ac34730, raw_cell=\"\n",
       "print(f\"dim_user: {dim_user.count()}\")\n",
       "print(f\"di..\" store_history=False silent=False shell_futures=True cell_id=None> result=None>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%timemem\n",
    "\n",
    "print(f\"dim_user: {dim_user.count()}\")\n",
    "print(f\"dim_age: {dim_age.count()}\")\n",
    "print(f\"dim_brand: {dim_brand.count()}\")\n",
    "print(f\"dim_category: {dim_category.count()}\")\n",
    "print(f\"dim_product: {dim_product.count()}\")\n",
    "print(f\"dim_date: {dim_date.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c991409d-c410-46b3-bf2c-b3acb4b5fb28",
   "metadata": {},
   "source": [
    "**Correct answers:**\n",
    "\n",
    "- `dim_user`: 3022290\n",
    "- `dim_age`: 10\n",
    "- `dim_brand`: 3444\n",
    "- `dim_category`: 13\n",
    "- `dim_product`: 166794\n",
    "- `dim_date`: 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1fa63f-101b-444d-9013-20c2947657b4",
   "metadata": {},
   "source": [
    "## 5. Build the Fact Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3523d3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Now it's time to build the fact table!\n",
    "\n",
    "Our goal in this step is to create a clean `fact_events` table that joins the events from the operational database to the dimension tables you've just built above.\n",
    "Along the way, we're going to enforce data quality and do a bit of data cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb231a1-f368-40b1-aa3c-e448c151bbec",
   "metadata": {},
   "source": [
    "### 5.1 Clean Events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b75d9cc-9eb4-47e5-85dd-f073172fc81c",
   "metadata": {},
   "source": [
    "Create `events_clean` by removing any record that \"does not make sense\".\n",
    "Specifically:\n",
    "\n",
    "- Start from the `df_events` DataFrame.\n",
    "- Keep only rows with non-null timestamps, `session_id`, and `product_id`.\n",
    "- Cast price to double; keep `NULL` prices (views/carts can be price-less) and non-negative values only.\n",
    "- Drop dates in the future.\n",
    "- Restrict to valid event types: `view`, `cart`, `purchase`, `remove`.\n",
    "\n",
    "**write some code here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "559ae17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "42418541"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================\n",
      "Wall time: 21.164 s\n",
      "RSS Δ: +0.00 MB\n",
      "Peak memory Δ: +0.00 MB (OS-dependent)\n",
      "======================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExecutionResult object at 712f3ac35e70, execution_count=None error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 712f3ac12050, raw_cell=\"# codecell_51ep7v (keep this id for tracking purpo..\" store_history=False silent=False shell_futures=True cell_id=None> result=42418541>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%timemem\n",
    "# codecell_51ep7v (keep this id for tracking purposes)\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from functools import reduce\n",
    "from operator import and_ as AND\n",
    "\n",
    "valid_types = [\"view\", \"cart\", \"purchase\", \"remove\"]\n",
    "\n",
    "events_clean = df_events\n",
    "\n",
    "events_clean = events_clean.filter(\n",
    "    (F.col(\"event_time\").isNotNull()) &\n",
    "    (F.col(\"session_id\").isNotNull()) &\n",
    "    (F.col(\"product_id\").isNotNull())\n",
    ")\n",
    "events_clean = events_clean.withColumn(\"price\", F.col(\"price\").cast(\"double\")) \\\n",
    "    .filter((F.col(\"price\").isNull()) | (F.col(\"price\") >= 0))\n",
    "\n",
    "events_clean = events_clean.filter(F.to_date(\"event_time\") <= F.current_date())\n",
    "events_clean = events_clean.filter(F.col(\"event_type\").isin(valid_types))\n",
    "\n",
    "# By the time we get to here, \"events_clean\" should conform to the specification above.\n",
    "\n",
    "events_clean.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94fd335",
   "metadata": {},
   "source": [
    "### 5.2 Cap Silly Prices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d6f0df-6101-4c96-a30a-03b498844a20",
   "metadata": {},
   "source": [
    "Next, let us check some statistics about prices and then decide what we want to do.\n",
    "\n",
    "What is the minimum, maximum, and average price in this database?\n",
    "\n",
    "**write some code here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a169cdb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 251:====================================================>  (25 + 1) / 26]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum: 0.0\n",
      "maximum: 257407.0\n",
      "average: 864.2732006942781\n",
      "======================================\n",
      "Wall time: 24.174 s\n",
      "RSS Δ: +0.00 MB\n",
      "Peak memory Δ: +0.00 MB (OS-dependent)\n",
      "======================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExecutionResult object at 712f393ebd60, execution_count=None error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 712f393ebd90, raw_cell=\"# codecell_52hg6x (keep this id for tracking purpo..\" store_history=False silent=False shell_futures=True cell_id=None> result=None>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%timemem\n",
    "# codecell_52hg6x (keep this id for tracking purposes)\n",
    "\n",
    "stats = events_clean.select(\n",
    "    F.min(\"price\").alias(\"minimum\"),\n",
    "    F.max(\"price\").alias(\"maximum\"),\n",
    "    F.avg(\"price\").alias(\"average\")\n",
    ").collect()[0]\n",
    "\n",
    "minimum = stats[\"minimum\"]\n",
    "maximum = stats[\"maximum\"]\n",
    "average = stats[\"average\"]\n",
    "\n",
    "# By the time we get to here, \"minimum\", \"maximum\", and \"average\" should conform to the specification above.\n",
    "\n",
    "print(f\"minimum: {minimum}\")\n",
    "print(f\"maximum: {maximum}\")\n",
    "print(f\"average: {average}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b48511",
   "metadata": {},
   "source": [
    "Wait, something's not right! \n",
    "The average price is 864.27 but the maximum seems suss...\n",
    "It is possible these high prices are just errors.\n",
    "\n",
    "For simplicity, let us assume a threshold value equal to 100x the average, and remove anything more than that.\n",
    "Filter `events_clean` as described.\n",
    "\n",
    "write some code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e9783cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "42351862"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================\n",
      "Wall time: 20.906 s\n",
      "RSS Δ: +0.00 MB\n",
      "Peak memory Δ: +0.00 MB (OS-dependent)\n",
      "======================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExecutionResult object at 712f393d1930, execution_count=None error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 712f393d0cd0, raw_cell=\"# codecell_52bf5d (keep this id for tracking purpo..\" store_history=False silent=False shell_futures=True cell_id=None> result=42351862>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%timemem\n",
    "# codecell_52bf5d (keep this id for tracking purposes)\n",
    "\n",
    "price_threshold = average * 100\n",
    "\n",
    "events_clean = events_clean.filter(\n",
    "    (F.col(\"price\").isNull()) | (F.col(\"price\") >= 0) & (F.col(\"price\") <= price_threshold)\n",
    ")\n",
    "\n",
    "# By the time we get to here, \"events_clean\" should conform to the specification above.\n",
    "\n",
    "events_clean.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac33e64b",
   "metadata": {},
   "source": [
    "Good, we still have about 42.4M records, but we've done some basic data cleaning.\n",
    "Let us continue..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3065688-4d49-4567-b7f5-eb866bf52441",
   "metadata": {},
   "source": [
    "### 5.3 Build Tiny Lookup Tables (LKPs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b5cb09-baf1-45c4-9b6b-0083c1d86b4b",
   "metadata": {},
   "source": [
    "Create lookup tables that help us connect `events_clean` with the dimension tables we created:\n",
    "\n",
    "- `user_lkp`: (`user_id` → `user_key`) from `dim_user`.\n",
    "- `prod_lkp`: (`product_id` → `product_key`, `brand_key`, `category_key`) from `dim_product`.\n",
    "- `date_lkp`: (`date` → `date_key`) from `dim_date`.\n",
    "- session-to-user bridge: use the raw `df_session` (`session_id`, `user_id`) CSV (not a dimension) to pull `user_id`.\n",
    "\n",
    "**Hint:** These LKPs are just calling `select` from the right sources with the right parameters.\n",
    "\n",
    "**write some code here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "056954dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9244421 3022290 166794 32\n",
      "======================================\n",
      "Wall time: 2.033 s\n",
      "RSS Δ: +0.00 MB\n",
      "Peak memory Δ: +0.00 MB (OS-dependent)\n",
      "======================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExecutionResult object at 712f3932ae30, execution_count=None error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 712f3932ba00, raw_cell=\"# codecell_53l2kp (keep this id for tracking purpo..\" store_history=False silent=False shell_futures=True cell_id=None> result=None>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%timemem\n",
    "# codecell_53l2kp (keep this id for tracking purposes)\n",
    "\n",
    "user_lkp = dim_user.select(\"user_id\", \"user_key\")\n",
    "\n",
    "prod_lkp = dim_product.select(\"product_id\", \"product_key\", \"brand_key\", \"category_key\")\n",
    "\n",
    "date_lkp = dim_date.select(\"date\", \"date_key\")\n",
    "\n",
    "session_bridge = df_session.select(\"session_id\", \"user_id\")\n",
    "\n",
    "# By the time we get to here, the following variables should conform to the specification above.\n",
    "\n",
    "print(session_bridge.count(), user_lkp.count(), prod_lkp.count(), date_lkp.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55b6b4e-0449-4db7-8971-98ceabe29eb9",
   "metadata": {},
   "source": [
    "### 5.4 Join Everything Together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad90901",
   "metadata": {},
   "source": [
    "Finally, join everything together to create `fact_events`.\n",
    "Follow the following steps:\n",
    "\n",
    "- Start from `clean events` with these columns: (`event_time`, `event_type`, `session_id`, `product_id`, `price`, `date`).\n",
    "- Join sessions first (to get `user_id`).\n",
    "- Then join product, date, and user.\n",
    "- Join with `dim_user` to find out the birthdate and compute user age at the day of the event in `age_on_event`.\n",
    "- Join with `dim_age` to find the age band based on `age_on_event`.\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "- You built the LKPs for a reason... use them.\n",
    "- Left, right, or natural joins?\n",
    "\n",
    "The final part above is a bit tricky, so we'll just give you the answer. But you'll need to figure out how it integrates with everything above.\n",
    "\n",
    "```\n",
    "        .withColumn(\"age_on_event\", F.floor(F.months_between(F.col(\"date\"), F.to_date(\"birthdate\"))/12))\n",
    "        .join(\n",
    "           dim_age.select(\"age_key\", \"age_band\", \"min_age\", \"max_age\"),\n",
    "           (\n",
    "               ((F.col(\"age_on_event\") > F.col(\"min_age\"))) &\n",
    "               ((F.col(\"age_on_event\") <= F.col(\"max_age\")))\n",
    "           ),\n",
    "           \"left\"\n",
    "       )\n",
    "```\n",
    "\n",
    "The final result (`fact_events`) should include the following columns:\n",
    "\n",
    "- `date_key`\n",
    "- `user_key`\n",
    "- `age_key`\n",
    "- `product_key`\n",
    "- `brand_key`\n",
    "- `category_key`\n",
    "- `session_id`\n",
    "- `event_time`\n",
    "- `event_type`\n",
    "- `price`\n",
    "\n",
    "**write some code here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4660de20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 279:===================================================>  (96 + 4) / 100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42351862\n",
      "======================================\n",
      "Wall time: 99.582 s\n",
      "RSS Δ: -24.38 MB\n",
      "Peak memory Δ: +0.12 MB (OS-dependent)\n",
      "======================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExecutionResult object at 712f3932a6b0, execution_count=None error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 712f3932a410, raw_cell=\"# codecell_54aaaa (keep this id for tracking purpo..\" store_history=False silent=False shell_futures=True cell_id=None> result=None>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%timemem\n",
    "# codecell_54aaaa (keep this id for tracking purposes)\n",
    "events_fact = events_clean.withColumn(\"date\", F.to_date(\"event_time\"))\n",
    "\n",
    "events_fact = events_fact.join(session_bridge, on=\"session_id\", how=\"left\")\n",
    "\n",
    "events_fact = events_fact.join(prod_lkp, on=\"product_id\", how=\"left\")\n",
    "\n",
    "events_fact = events_fact.join(date_lkp, on=\"date\", how=\"left\")\n",
    "\n",
    "events_fact = events_fact.join(user_lkp, on=\"user_id\", how=\"left\")\n",
    "events_fact = events_fact.join(dim_user.select(\"user_id\", \"birthdate\"), on=\"user_id\", how=\"left\")\n",
    "\n",
    "events_fact = events_fact.withColumn(\n",
    "    \"age_on_event\", F.floor(F.months_between(F.col(\"date\"), F.to_date(F.col(\"birthdate\")))/12)\n",
    ")\n",
    "\n",
    "events_fact = events_fact.join(\n",
    "    dim_age.select(\"age_key\", \"min_age\", \"max_age\"),\n",
    "    (F.col(\"age_on_event\") > F.col(\"min_age\")) & (F.col(\"age_on_event\") <= F.col(\"max_age\")),\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "fact_events = events_fact.select(\n",
    "    \"date_key\",\n",
    "    \"user_key\",\n",
    "    \"age_key\",\n",
    "    \"product_key\",\n",
    "    \"brand_key\",\n",
    "    \"category_key\",\n",
    "    \"session_id\",\n",
    "    \"event_time\",\n",
    "    \"event_type\",\n",
    "    \"price\"\n",
    ")\n",
    "\n",
    "# By the time we get to here, \"fact_events\" should conform to the specification above.\n",
    "\n",
    "print(fact_events.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "db3427cf-f61b-4399-8f27-16526b0e882a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42418541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42351862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 315:=====================================================>(99 + 1) / 100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42351862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print(df_events.count())\n",
    "print(events_clean.count())\n",
    "print(fact_events.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abdfc6c",
   "metadata": {},
   "source": [
    "Congrats, you've done it!\n",
    "You've created the fact table successfuly! 🚀\n",
    "\n",
    "Here is the summary of the schema:\n",
    "\n",
    "- `date_key` (FK → `dim_date`)\n",
    "- `user_key` (FK → `dim_user`)\n",
    "- `age_key`  (FK → `dim_age`)\n",
    "- `product_key` (FK → `dim_product`)\n",
    "- `brand_key` (FK → `dim_brand`)\n",
    "- `category_key` (FK → `dim_category`)\n",
    "- `session_id` (STRING, business key, kept directly in this table)\n",
    "- `event_time` (TIMESTAMP)\n",
    "- `event_tpe` (STRING)\n",
    "- `price` (DOUBLE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aa5c0a-b7ab-4340-9b05-1dc615444254",
   "metadata": {},
   "source": [
    "## 6. Export the Fact Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ebb4ea",
   "metadata": {},
   "source": [
    "You now have a shiny `fact_events` table!\n",
    "But how should you store it?\n",
    "(Remember our discussion in class about row vs. column representations?)\n",
    "\n",
    "Let's store `fact_events` in a few different ways and compare data sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb22a99-f504-4487-97c3-324554333091",
   "metadata": {},
   "source": [
    "First, let's try writing out as CSV files, both compressed and uncompressed, per below.\n",
    "\n",
    "Note that in Spark, we specify the output _directory_, which is then populated with many \"part\" files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cb42721a-f950-411d-93ce-2069629f019a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/05 15:29:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:29:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:29:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:29:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:29:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:29:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:29:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:29:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:29:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:29:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:29:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:29:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:29:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:29:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:29:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:29:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:29:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:29:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:29:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:29:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:29:15 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:29:15 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:29:15 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:29:15 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:29:15 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:29:15 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:29:15 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:29:15 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:29:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:29:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:29:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:29:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:29:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:29:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:29:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:29:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:29:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:29:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:29:20 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:29:20 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:29:20 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:29:20 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:29:20 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:29:20 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:29:21 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:29:21 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:29:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:29:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:29:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:29:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:29:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:29:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:29:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:29:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:29:55 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:29:55 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:01 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:01 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:01 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:01 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:01 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:01 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:01 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:01 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:01 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:01 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:02 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:02 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:02 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:02 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:02 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:02 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:02 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:02 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:02 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:02 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:02 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:02 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:02 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:02 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:02 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:02 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:02 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:02 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:07 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:07 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:07 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:07 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:32:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Stage 379:===============>                                        (8 + 8) / 29]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4639.331s][warning][gc,alloc] Executor task launch worker for task 9.0 in stage 379.0 (TID 1468): Retried waiting for GCLocker too often allocating 2097154 words\n",
      "[4639.356s][warning][gc,alloc] Executor task launch worker for task 8.0 in stage 379.0 (TID 1467): Retried waiting for GCLocker too often allocating 2097154 words\n",
      "[4639.374s][warning][gc,alloc] Executor task launch worker for task 11.0 in stage 379.0 (TID 1470): Retried waiting for GCLocker too often allocating 2097154 words\n",
      "[4639.374s][warning][gc,alloc] Executor task launch worker for task 9.0 in stage 379.0 (TID 1468): Retried waiting for GCLocker too often allocating 2097154 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/05 15:33:08 WARN TaskMemoryManager: Failed to allocate a page (16777216 bytes), try again.\n",
      "25/11/05 15:33:08 WARN TaskMemoryManager: Failed to allocate a page (16777216 bytes), try again.\n",
      "25/11/05 15:33:08 WARN TaskMemoryManager: Failed to allocate a page (16777216 bytes), try again.\n",
      "25/11/05 15:33:08 WARN TaskMemoryManager: Failed to allocate a page (16777216 bytes), try again.\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "fact_events.write.mode(\"overwrite\").option(\"header\", True).csv(BASE_DIR + \"/fact_events.csv\")\n",
    "fact_events.write.mode(\"overwrite\").option(\"header\", True).option(\"compression\", \"snappy\").csv(BASE_DIR + \"/fact_events.csv.snappy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bb2541-f412-437f-8b1f-d973ab722ab5",
   "metadata": {},
   "source": [
    "Let's then try Parquet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4e9721b6-d435-4438-bf06-131a8d0a4384",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/05 15:36:54 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:36:54 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:36:54 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:36:54 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:36:54 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:36:54 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:36:54 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:36:54 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:36:54 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:36:54 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:36:55 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:36:55 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:36:55 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:36:55 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:36:55 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:36:55 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:36:55 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:36:55 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:36:55 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:36:55 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:36:59 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:36:59 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:36:59 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:37:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:37:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:37:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:37:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:37:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:37:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:37:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:37:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:37:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:37:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:37:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:37:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:37:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:37:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:37:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:37:01 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:37:01 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:37:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:37:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:37:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:37:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:37:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:37:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:37:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:37:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:37:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:37:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/05 15:38:39 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/05 15:38:46 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/05 15:38:49 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/05 15:38:51 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/05 15:38:53 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/05 15:38:55 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/05 15:38:55 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/05 15:38:59 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/05 15:39:01 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/05 15:39:03 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/05 15:39:03 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/05 15:39:04 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/05 15:39:05 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/05 15:39:06 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/05 15:39:06 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/05 15:39:07 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/05 15:39:08 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/05 15:39:09 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/05 15:39:10 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/05 15:39:11 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/05 15:39:16 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/05 15:39:20 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/05 15:39:23 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/05 15:39:24 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/05 15:39:25 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/05 15:39:26 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/05 15:39:26 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/05 15:39:28 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/05 15:39:30 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/05 15:39:30 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/05 15:39:32 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/05 15:39:32 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/05 15:39:34 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/05 15:39:34 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/05 15:39:36 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/05 15:39:36 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/05 15:39:36 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/05 15:39:39 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/05 15:39:40 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/05 15:39:40 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/05 15:39:42 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "fact_events.write.mode(\"overwrite\").parquet(BASE_DIR + \"/fact_events.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757d8161-114a-4896-b5b9-aa7a27c02e76",
   "metadata": {},
   "source": [
    "Let's compare the output sizes using the following bit of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "453bd520-4a21-4161-b6c2-6758bb6546e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/justine/de1-website/DE1/labs-final/lab2-assignment/csv/fact_events.csv: 4.3 GB\n",
      "/home/justine/de1-website/DE1/labs-final/lab2-assignment/csv/fact_events.csv.snappy: 1.2 GB\n",
      "/home/justine/de1-website/DE1/labs-final/lab2-assignment/csv/fact_events.parquet: 1.0 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for f in [BASE_DIR + \"/fact_events.csv\", BASE_DIR + \"/fact_events.csv.snappy\", BASE_DIR + \"/fact_events.parquet\"]:\n",
    "    try:\n",
    "        size = sum(os.path.getsize(os.path.join(dp, fn))\n",
    "                   for dp, dn, filenames in os.walk(f)\n",
    "                   for fn in filenames)\n",
    "        print(f\"{f}: {size/(1024*1024*1024):.1f} GB\")\n",
    "    except FileNotFoundError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d796fa9-cad5-4ca6-b6cd-cbae63cdc96e",
   "metadata": {},
   "source": [
    "**your answers below!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56df9e6-a6bd-4ebd-a692-63c4bd590475",
   "metadata": {},
   "source": [
    "// qcell_6a9876 (keep this id for tracking purposes)\n",
    "\n",
    "- **Size of CSV output, no compression:** 4.3  GB\n",
    "- **Size of CSV output, Snappy compression:** 1.2 GB\n",
    "- **Size of Parquet output:** 1.0 GB "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371b7369-6ce3-455a-917b-bc002e112da4",
   "metadata": {},
   "source": [
    "**Answer the following question:**\n",
    "\n",
    "Q6.1 Why is columnar storage (Parquet) usually much smaller?\n",
    "\n",
    "Q6.2 Which format is better for analytical queries and why?\n",
    "\n",
    "**your answers below!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad1b29b-375e-4143-b844-cd40c3183e0f",
   "metadata": {},
   "source": [
    "// qcell_6b1234 (keep this id for tracking purposes)\n",
    "\n",
    "**Q6.1 Answer:**\n",
    "Parquet files are much smaller because they store data column by column. This makes compression more efficient, especially when values repeat a lot in a column. Also, Parquet uses encoding and metadata optimizations, whereas CSV just stores everything as plain text, which takes more space.\n",
    "\n",
    "**Q6.2 Answer:**\n",
    "I would say Parquet is better for analytical queries because it only reads the columns you need, so queries run faster. Also, it is compressed and optimized for filtering and aggregation, which reduces I/O and memory usage. CSV is easier to handle for small files, but for big datasets, Parquet is definitely more efficient (and smaller size of storage output)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f5ab3d-73dd-4bd9-9722-4f07e6b913c0",
   "metadata": {},
   "source": [
    "## 7. Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568bb473",
   "metadata": {},
   "source": [
    "Details about the Submission of this assignment are outlined in the helper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "89841125",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/de1-env/lib/python3.10/site-packages/pyspark/sql/session.py:2023\u001b[0m, in \u001b[0;36mSparkSession.stop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2010\u001b[0m \u001b[38;5;124;03mStop the underlying :class:`SparkContext`.\u001b[39;00m\n\u001b[1;32m   2011\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2019\u001b[0m \u001b[38;5;124;03m>>> spark.stop()  # doctest: +SKIP\u001b[39;00m\n\u001b[1;32m   2020\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2021\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SQLContext\n\u001b[0;32m-> 2023\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2024\u001b[0m \u001b[38;5;66;03m# We should clean the default session up. See SPARK-23228.\u001b[39;00m\n\u001b[1;32m   2025\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/de1-env/lib/python3.10/site-packages/pyspark/core/context.py:684\u001b[0m, in \u001b[0;36mSparkContext.stop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_jsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    683\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 684\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JError:\n\u001b[1;32m    686\u001b[0m         \u001b[38;5;66;03m# Case: SPARK-18523\u001b[39;00m\n\u001b[1;32m    687\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    688\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to cleanly shutdown Spark JVM process.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    689\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m It is possible that the process has crashed,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    690\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m been killed or may also be in a zombie state.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    691\u001b[0m             \u001b[38;5;167;01mRuntimeWarning\u001b[39;00m,\n\u001b[1;32m    692\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/de1-env/lib/python3.10/site-packages/py4j/java_gateway.py:1361\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1363\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/miniconda3/envs/de1-env/lib/python3.10/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m~/miniconda3/envs/de1-env/lib/python3.10/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/miniconda3/envs/de1-env/lib/python3.10/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/miniconda3/envs/de1-env/lib/python3.10/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================\n",
      "Wall time: 0.094 s\n",
      "RSS Δ: +2.75 MB\n",
      "Peak memory Δ: +0.00 MB (OS-dependent)\n",
      "======================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExecutionResult object at 712f3044ca30, execution_count=None error_before_exec=None error_in_exec=[Errno 111] Connection refused info=<ExecutionInfo object at 712f3044c9a0, raw_cell=\"spark.stop()\n",
       "\" store_history=False silent=False shell_futures=True cell_id=None> result=None>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%timemem\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c95cb45",
   "metadata": {},
   "source": [
    "## Deliverables\n",
    "- This notebook with all code cells executed.\n",
    "- A brief `REPORT.md` with: inputs, assumptions, plan screenshots, quality results, and performance choices.\n",
    "- Output folder with Parquet sample (≤20 MB).\n",
    "\n",
    "## Evaluation\n",
    "- Correctness and clarity of pipeline (40%).\n",
    "- Data‑quality gates and rationale (20%).\n",
    "- Performance reasoning and plan analysis (20%).\n",
    "- Reproducibility and organization (20%).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46acfb1",
   "metadata": {},
   "source": [
    "## Performance notes\n",
    "- Record `spark.sql.shuffle.partitions` and justify your value.\n",
    "- Show one example of avoiding UDFs by using built‑ins.\n",
    "- If you use broadcast join, explain why it is safe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8c913a76-5e53-4123-8628-e9534be82f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'200'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5583ac",
   "metadata": {},
   "source": [
    "## Reproducibility checklist\n",
    "- List Spark version and key configs.\n",
    "- Fix time zone to UTC.\n",
    "- Control randomness if used.\n",
    "- Provide exact commands to run the notebook end‑to‑end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f1b99c3e-eefd-44cd-a47c-d1175237e3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "67789e75-836c-4963-939a-90174139776e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 4.0.1\n",
      "Spark session time zone: UTC\n",
      "BASE_DIR: /home/justine/de1-website/DE1/labs-final/lab2-assignment/csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 428:====================================================>  (25 + 1) / 26]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframes loaded: 3022290 42418541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print(\"Spark version:\", spark.version)\n",
    "print(\"Spark session time zone:\", spark.conf.get(\"spark.sql.session.timeZone\"))\n",
    "print(\"BASE_DIR:\", BASE_DIR)\n",
    "print(\"Dataframes loaded:\", df_user.count(), df_events.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "359340bf-26f6-4bd1-b9de-27af81ea04e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('spark.driver.port', '46809'), ('spark.driver.host', '10.255.255.254'), ('spark.rdd.compress', 'True'), ('spark.hadoop.fs.s3a.vectored.read.min.seek.size', '128K'), ('spark.app.name', 'de1-lab2-assignment'), ('spark.executor.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-modules=jdk.incubator.vector --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dio.netty.tryReflectionSetAccessible=true'), ('spark.sql.artifact.isolation.enabled', 'false'), ('spark.sql.warehouse.dir', 'file:/home/justine/Data_engineering/lab2_assignment/spark-warehouse'), ('spark.app.startTime', '1762348551846'), ('spark.master', 'local[*]'), ('spark.app.submitTime', '1762348551222'), ('spark.executor.id', 'driver'), ('spark.submit.pyFiles', ''), ('spark.driver.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-modules=jdk.incubator.vector --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dio.netty.tryReflectionSetAccessible=true'), ('spark.hadoop.fs.s3a.vectored.read.max.merged.size', '2M'), ('spark.app.id', 'local-1762348552951'), ('spark.submit.deployMode', 'client'), ('spark.serializer.objectStreamReset', '100'), ('spark.ui.showConsoleProgress', 'true')]\n"
     ]
    }
   ],
   "source": [
    "print(spark.sparkContext.getConf().getAll())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffc4add-63dc-493b-9554-ec6c180ef907",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (de1-env)",
   "language": "python",
   "name": "de1-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
