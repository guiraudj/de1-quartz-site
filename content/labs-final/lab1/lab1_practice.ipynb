{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf709236-ad39-4688-ae48-8ee22ffccabe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.10.18 (main, Jun  5 2025, 13:14:17) [GCC 11.2.0]\n",
      "Spark: 4.0.1\n"
     ]
    }
   ],
   "source": [
    "import os, sys, datetime, pathlib\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "print(\"Python:\", sys.version)\n",
    "spark = SparkSession.builder.appName(\"de1-lab1\").getOrCreate()\n",
    "print(\"Spark:\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f32085c-c126-4eaa-90a2-99a740a81813",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/mnt/c/Users/Justine/Data_engineering/lab1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c284d87-774f-4f9c-a0b6-730d6b178c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/Justine/Data_engineering/lab1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Affiche le chemin absolu du répertoire courant\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b61df04-5e7a-477b-867d-77be5d5e9227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 2700\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- value: double (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n",
      "+---+-----------+-----+----------------------------------------------------------------------------+\n",
      "|id |category   |value|text                                                                        |\n",
      "+---+-----------+-----+----------------------------------------------------------------------------+\n",
      "|0  |toys       |48.47|metrics ui data elt row columnar reduce warehouse shuffle join spark elt    |\n",
      "|1  |books      |39.9 |metrics row lake aggregate columnar data reduce row columnar filter         |\n",
      "|2  |grocery    |7.96 |lake join partition scala elt data                                          |\n",
      "|3  |electronics|5.15 |spark scala elt filter join columnar lake lake plan warehouse columnar spark|\n",
      "|4  |toys       |44.87|aggregate metrics row row filter lake map metrics columnar spark            |\n",
      "+---+-----------+-----+----------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "src_a = \"data/lab1_dataset_a.csv\"\n",
    "src_b = \"data/lab1_dataset_b.csv\"\n",
    "df_a = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(src_a)\n",
    "df_b = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(src_b)\n",
    "df = df_a.unionByName(df_b)\n",
    "df.cache()\n",
    "print(\"Rows:\", df.count())\n",
    "df.printSchema()\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "324afbe8-326f-4f9c-b31f-795a18fa7f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def export_spark_metrics(spark, run_id, note=\"\", output_csv=\"spark_metrics.csv\"):\n",
    "    \"\"\"\n",
    "    Exporte les métriques Spark (stages) dans un CSV --> toutes les métriques \n",
    "    - spark: SparkSession active\n",
    "    - run_id: identifiant de ton exécution (ex: \"r1\", \"baseline\", etc.)\n",
    "    - note: commentaire ou variante testée (ex: \"broadcast join\", etc.)\n",
    "    - output_csv: nom du fichier de sortie\n",
    "    \"\"\"\n",
    "    ui_url = spark.sparkContext.uiWebUrl\n",
    "    app_id = spark.sparkContext.applicationId\n",
    "    \n",
    "    if not ui_url:\n",
    "        print(\" Pas d'UI Spark détectée (aucun job en cours ?)\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Récupérer les infos de stages\n",
    "        stages_url = f\"{ui_url}/api/v1/applications/{app_id}/stages\"\n",
    "        stages = requests.get(stages_url).json()\n",
    "    except Exception as e:\n",
    "        print(f\" Erreur API Spark: {e}\")\n",
    "        return\n",
    "    \n",
    "    data = []\n",
    "    for s in stages:\n",
    "        stage_id = s.get(\"stageId\")\n",
    "        job_ids = s.get(\"jobIds\", [])\n",
    "        metrics = s.get(\"executorSummary\", {})\n",
    "        \n",
    "        # Métriques de base\n",
    "        input_bytes = s.get(\"inputBytes\", 0)\n",
    "        shuffle_read = s.get(\"shuffleReadBytes\", 0)\n",
    "        shuffle_write = s.get(\"shuffleWriteBytes\", 0)\n",
    "        num_tasks = s.get(\"numTasks\", 0)\n",
    "        completion_time = s.get(\"completionTime\", None)\n",
    "        \n",
    "        # Fallback pour Spark < 3.x\n",
    "        if not completion_time and \"completionTime\" in s.get(\"completionTime\", {}):\n",
    "            completion_time = s[\"completionTime\"]\n",
    "        \n",
    "        data.append({\n",
    "            \"run_id\": run_id,\n",
    "            \"job_id\": job_ids[0] if job_ids else None,\n",
    "            \"stage_id\": stage_id,\n",
    "            \"task\": s.get(\"name\", \"\"),\n",
    "            \"note\": note,\n",
    "            \"files_read\": s.get(\"inputRecords\", 0),\n",
    "            \"input_size_bytes\": input_bytes,\n",
    "            \"shuffle_read_bytes\": shuffle_read,\n",
    "            \"shuffle_write_bytes\": shuffle_write,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    if df.empty:\n",
    "        print(\" Aucune métrique trouvée (aucun stage terminé ?)\")\n",
    "        return\n",
    "\n",
    "    # Écrire ou ajouter au CSV\n",
    "    if os.path.exists(output_csv):\n",
    "        df.to_csv(output_csv, mode=\"a\", header=False, index=False)\n",
    "    else:\n",
    "        df.to_csv(output_csv, index=False)\n",
    "\n",
    "    print(f\" {len(df)} lignes exportées vers {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b54890fe-26a5-4d50-a8ee-e4fcd8385b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def export_spark_metrics(spark, run_id, task=\"\", note=\"\", output_csv=\"metrics/lab1_metrics.csv\"):\n",
    "    \"\"\"\n",
    "    Exporte uniquement les métriques essentielles Spark (stages) dans un CSV.\n",
    "    Colonnes : run_id, task, note, files_read, input_size_bytes, shuffle_read_bytes, shuffle_write_bytes, timestamp\n",
    "    \"\"\"\n",
    "    ui_url = spark.sparkContext.uiWebUrl\n",
    "    app_id = spark.sparkContext.applicationId\n",
    "    \n",
    "    if not ui_url:\n",
    "        print(\" Pas d'UI Spark détectée.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        stages = requests.get(f\"{ui_url}/api/v1/applications/{app_id}/stages\").json()\n",
    "    except Exception as e:\n",
    "        print(f\" Erreur API Spark: {e}\")\n",
    "        return\n",
    "    \n",
    "    if not stages:\n",
    "        print(\"Aucun stage trouvé.\")\n",
    "        return\n",
    "\n",
    "    data = []\n",
    "    for s in stages:\n",
    "        data.append({\n",
    "            \"run_id\": run_id,\n",
    "            \"task\": task,\n",
    "            \"note\": note,\n",
    "            \"files_read\": s.get(\"inputRecords\", 0),\n",
    "            \"input_size_bytes\": s.get(\"inputBytes\", 0),\n",
    "            \"shuffle_read_bytes\": s.get(\"shuffleReadBytes\", 0),\n",
    "            \"shuffle_write_bytes\": s.get(\"shuffleWriteBytes\", 0),\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    if df.empty:\n",
    "        print(\"Aucune métrique trouvée.\")\n",
    "        return\n",
    "\n",
    "    # Crée le dossier si nécessaire\n",
    "    os.makedirs(os.path.dirname(output_csv), exist_ok=True) if os.path.dirname(output_csv) else None\n",
    "\n",
    "    if os.path.exists(output_csv):\n",
    "        df.to_csv(output_csv, mode=\"a\", header=False, index=False)\n",
    "    else:\n",
    "        df.to_csv(output_csv, index=False)\n",
    "\n",
    "    print(f\" {len(df)} lignes exportées vers {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e425ae8a-487b-4e7f-a2a0-f92dec24777f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lake', 1215),\n",
       " ('scala', 1200),\n",
       " ('elt', 1199),\n",
       " ('metrics', 1190),\n",
       " ('row', 1183),\n",
       " ('join', 1169),\n",
       " ('warehouse', 1168),\n",
       " ('shuffle', 1160),\n",
       " ('ui', 1145),\n",
       " ('aggregate', 1144)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD pipeline: tokenize 'text' column and count tokens\n",
    "rdd = df.select(\"text\").rdd.flatMap(lambda row: (row[0] or \"\").lower().split())\n",
    "pair = rdd.map(lambda t: (t, 1))\n",
    "counts = pair.reduceByKey(lambda a,b: a+b)\n",
    "top_rdd = counts.sortBy(lambda kv: (-kv[1], kv[0])).take(10)\n",
    "top_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fc4dcb9-7a8b-4703-9d84-395316ed077b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote outputs/top10_rdd.csv\n"
     ]
    }
   ],
   "source": [
    "# Save as CSV (token,count)\n",
    "pathlib.Path(\"outputs\").mkdir(exist_ok=True)\n",
    "with open(\"outputs/top10_rdd.csv\",\"w\",encoding=\"utf-8\") as f:\n",
    "    f.write(\"token,count\\n\")\n",
    "    for t,c in top_rdd:\n",
    "        f.write(f\"{t},{c}\\n\")\n",
    "print(\"Wrote outputs/top10_rdd.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6140a59-645f-40c1-851f-ba6e23c069ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved proof/plan_rdd.txt\n"
     ]
    }
   ],
   "source": [
    "# Trigger an action and record a textual plan for evidence\n",
    "_ = counts.count()\n",
    "plan_rdd = df._jdf.queryExecution().executedPlan().toString()\n",
    "pathlib.Path(\"proof\").mkdir(exist_ok=True)\n",
    "with open(\"proof/plan_rdd.txt\",\"w\") as f:\n",
    "    f.write(str(datetime.datetime.now()) + \"\\n\\n\")\n",
    "    f.write(plan_rdd)\n",
    "print(\"Saved proof/plan_rdd.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90983c25-377b-40bc-b138-f41a9a484834",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|token    |count|\n",
      "+---------+-----+\n",
      "|lake     |1215 |\n",
      "|scala    |1200 |\n",
      "|elt      |1199 |\n",
      "|metrics  |1190 |\n",
      "|row      |1183 |\n",
      "|join     |1169 |\n",
      "|warehouse|1168 |\n",
      "|shuffle  |1160 |\n",
      "|ui       |1145 |\n",
      "|aggregate|1144 |\n",
      "+---------+-----+\n",
      "\n",
      "Wrote outputs/top10_df.csv\n"
     ]
    }
   ],
   "source": [
    "tokens = F.explode(F.split(F.lower(F.col(\"text\")), \"\\\\s+\")).alias(\"token\")\n",
    "df_tokens = df.select(tokens).where(F.col(\"token\") != \"\")\n",
    "agg_df = df_tokens.groupBy(\"token\").agg(F.count(\"*\").alias(\"count\"))\n",
    "top_df = agg_df.orderBy(F.desc(\"count\"), F.asc(\"token\")).limit(10)\n",
    "top_df.show(truncate=False)\n",
    "top_df.coalesce(1).write.mode(\"overwrite\").option(\"header\",\"true\").csv(\"outputs/top10_df_tmp\")\n",
    "# move single part file to stable path\n",
    "import glob, shutil\n",
    "part = glob.glob(\"outputs/top10_df_tmp/part*\")[0]\n",
    "shutil.copy(part, \"outputs/top10_df.csv\")\n",
    "print(\"Wrote outputs/top10_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f68c6cab-596d-4b4e-984e-e3e66f122a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved proof/plan_df.txt\n"
     ]
    }
   ],
   "source": [
    "plan_df = top_df._jdf.queryExecution().executedPlan().toString()\n",
    "with open(\"proof/plan_df.txt\",\"w\") as f:\n",
    "    f.write(str(datetime.datetime.now()) + \"\\n\\n\")\n",
    "    f.write(plan_df)\n",
    "print(\"Saved proof/plan_df.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47786c38-e7c8-4dc1-928a-ca887d14b47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (9)\n",
      "+- HashAggregate (8)\n",
      "   +- Exchange (7)\n",
      "      +- HashAggregate (6)\n",
      "         +- InMemoryTableScan (1)\n",
      "               +- InMemoryRelation (2)\n",
      "                     +- Union (5)\n",
      "                        :- Scan csv  (3)\n",
      "                        +- Scan csv  (4)\n",
      "\n",
      "\n",
      "(1) InMemoryTableScan\n",
      "Output [2]: [category#18, value#19]\n",
      "Arguments: [category#18, value#19]\n",
      "\n",
      "(2) InMemoryRelation\n",
      "Arguments: [id#17, category#18, value#19, text#20], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "\n",
      "(3) Scan csv \n",
      "Output [4]: [id#17, category#18, value#19, text#20]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/mnt/c/Users/Justine/Data_engineering/lab1/data/lab1_dataset_a.csv]\n",
      "ReadSchema: struct<id:int,category:string,value:double,text:string>\n",
      "\n",
      "(4) Scan csv \n",
      "Output [4]: [id#38, category#39, value#40, text#41]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/mnt/c/Users/Justine/Data_engineering/lab1/data/lab1_dataset_b.csv]\n",
      "ReadSchema: struct<id:int,category:string,value:double,text:string>\n",
      "\n",
      "(5) Union\n",
      "\n",
      "(6) HashAggregate\n",
      "Input [2]: [category#18, value#19]\n",
      "Keys [1]: [category#18]\n",
      "Functions [1]: [partial_sum(value#19)]\n",
      "Aggregate Attributes [1]: [sum#752]\n",
      "Results [2]: [category#18, sum#753]\n",
      "\n",
      "(7) Exchange\n",
      "Input [2]: [category#18, sum#753]\n",
      "Arguments: hashpartitioning(category#18, 200), ENSURE_REQUIREMENTS, [plan_id=392]\n",
      "\n",
      "(8) HashAggregate\n",
      "Input [2]: [category#18, sum#753]\n",
      "Keys [1]: [category#18]\n",
      "Functions [1]: [sum(value#19)]\n",
      "Aggregate Attributes [1]: [sum(value#19)#691]\n",
      "Results [2]: [category#18, sum(value#19)#691 AS sum_value#686]\n",
      "\n",
      "(9) AdaptiveSparkPlan\n",
      "Output [2]: [category#18, sum_value#686]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (9)\n",
      "+- HashAggregate (8)\n",
      "   +- Exchange (7)\n",
      "      +- HashAggregate (6)\n",
      "         +- InMemoryTableScan (1)\n",
      "               +- InMemoryRelation (2)\n",
      "                     +- Union (5)\n",
      "                        :- Scan csv  (3)\n",
      "                        +- Scan csv  (4)\n",
      "\n",
      "\n",
      "(1) InMemoryTableScan\n",
      "Output [2]: [category#18, value#19]\n",
      "Arguments: [category#18, value#19]\n",
      "\n",
      "(2) InMemoryRelation\n",
      "Arguments: [id#17, category#18, value#19, text#20], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "\n",
      "(3) Scan csv \n",
      "Output [4]: [id#17, category#18, value#19, text#20]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/mnt/c/Users/Justine/Data_engineering/lab1/data/lab1_dataset_a.csv]\n",
      "ReadSchema: struct<id:int,category:string,value:double,text:string>\n",
      "\n",
      "(4) Scan csv \n",
      "Output [4]: [id#38, category#39, value#40, text#41]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/mnt/c/Users/Justine/Data_engineering/lab1/data/lab1_dataset_b.csv]\n",
      "ReadSchema: struct<id:int,category:string,value:double,text:string>\n",
      "\n",
      "(5) Union\n",
      "\n",
      "(6) HashAggregate\n",
      "Input [2]: [category#18, value#19]\n",
      "Keys [1]: [category#18]\n",
      "Functions [1]: [partial_sum(value#19)]\n",
      "Aggregate Attributes [1]: [sum#924]\n",
      "Results [2]: [category#18, sum#925]\n",
      "\n",
      "(7) Exchange\n",
      "Input [2]: [category#18, sum#925]\n",
      "Arguments: hashpartitioning(category#18, 200), ENSURE_REQUIREMENTS, [plan_id=523]\n",
      "\n",
      "(8) HashAggregate\n",
      "Input [2]: [category#18, sum#925]\n",
      "Keys [1]: [category#18]\n",
      "Functions [1]: [sum(value#19)]\n",
      "Aggregate Attributes [1]: [sum(value#19)#863]\n",
      "Results [2]: [category#18, sum(value#19)#863 AS sum_value#860]\n",
      "\n",
      "(9) AdaptiveSparkPlan\n",
      "Output [2]: [category#18, sum_value#860]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n",
      "Open Spark UI at http://localhost:4040 while each job runs and record metrics into lab1_metrics_log.csv\n"
     ]
    }
   ],
   "source": [
    "# Case A: select all columns then aggregate on 'category'\n",
    "all_cols = df.select(\"*\").groupBy(\"category\").agg(F.sum(\"value\").alias(\"sum_value\"))\n",
    "all_cols.explain(\"formatted\")\n",
    "_ = all_cols.count()  # trigger\n",
    "\n",
    "# Case B: minimal projection then aggregate\n",
    "proj = df.select(\"category\",\"value\").groupBy(\"category\").agg(F.sum(\"value\").alias(\"sum_value\"))\n",
    "proj.explain(\"formatted\")\n",
    "_ = proj.count()  # trigger\n",
    "\n",
    "print(\"Open Spark UI at http://localhost:4040 while each job runs and record metrics into lab1_metrics_log.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4cc0a957-4f94-4d45-a95d-64d914036852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 151 lignes exportées vers metrics/lab1_metrics.csv\n",
      "+---------+-----+\n",
      "|token    |count|\n",
      "+---------+-----+\n",
      "|lake     |1215 |\n",
      "|scala    |1200 |\n",
      "|elt      |1199 |\n",
      "|metrics  |1190 |\n",
      "|row      |1183 |\n",
      "|join     |1169 |\n",
      "|warehouse|1168 |\n",
      "|shuffle  |1160 |\n",
      "|ui       |1145 |\n",
      "|aggregate|1144 |\n",
      "+---------+-----+\n",
      "\n",
      " 154 lignes exportées vers metrics/lab1_metrics.csv\n",
      " 160 lignes exportées vers metrics/lab1_metrics.csv\n",
      " 166 lignes exportées vers metrics/lab1_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "#Pour enregistrer les métriques de chaque fonction\n",
    "# Top-N RDD\n",
    "top_rdd = counts.sortBy(lambda kv: (-kv[1], kv[0])).take(10)\n",
    "export_spark_metrics(spark, run_id=\"r1\", task=\"topN_rdd\")\n",
    "\n",
    "# Top-N DataFrame\n",
    "top_df.show(truncate=False)\n",
    "export_spark_metrics(spark, run_id=\"r1\", task=\"topN_df\")\n",
    "\n",
    "# Projection all columns\n",
    "_ = all_cols.count()\n",
    "export_spark_metrics(spark, run_id=\"r1\", task=\"projection_all_cols\")\n",
    "\n",
    "# Projection minimal columns\n",
    "_ = proj.count()\n",
    "export_spark_metrics(spark, run_id=\"r1\", task=\"projection_min_cols\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43f870e-b370-47d9-9983-38fe6e2aff54",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print(\"Spark session stopped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0912b92-f1bc-40ed-8dee-637de685daf5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (de1-env)",
   "language": "python",
   "name": "de1-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
