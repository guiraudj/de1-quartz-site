{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed7d51e7",
   "metadata": {},
   "source": [
    "# DE1 — Lab 2: PostgreSQL → Star Schema ETL\n",
    "> Author : Badr TAJINI - Data Engineering I - ESIEE 2025-2026\n",
    "---\n",
    "\n",
    "\n",
    "Execute all cells. Attach evidence and fill metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6272fb2b",
   "metadata": {},
   "source": [
    "## 0. Setup and schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2a9a778c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F, types as T\n",
    "spark = SparkSession.builder.appName(\"de1-lab2\").getOrCreate()\n",
    "base = \"data/\"\n",
    "# Explicit schemas\n",
    "customers_schema = T.StructType([\n",
    "    T.StructField(\"customer_id\", T.IntegerType(), False),\n",
    "    T.StructField(\"name\", T.StringType(), True),\n",
    "    T.StructField(\"email\", T.StringType(), True),\n",
    "    T.StructField(\"created_at\", T.TimestampType(), True),\n",
    "])\n",
    "brands_schema = T.StructType([\n",
    "    T.StructField(\"brand_id\", T.IntegerType(), False),\n",
    "    T.StructField(\"brand_name\", T.StringType(), True),\n",
    "])\n",
    "categories_schema = T.StructType([\n",
    "    T.StructField(\"category_id\", T.IntegerType(), False),\n",
    "    T.StructField(\"category_name\", T.StringType(), True),\n",
    "])\n",
    "products_schema = T.StructType([\n",
    "    T.StructField(\"product_id\", T.IntegerType(), False),\n",
    "    T.StructField(\"product_name\", T.StringType(), True),\n",
    "    T.StructField(\"brand_id\", T.IntegerType(), True),\n",
    "    T.StructField(\"category_id\", T.IntegerType(), True),\n",
    "    T.StructField(\"price\", T.DoubleType(), True),\n",
    "])\n",
    "orders_schema = T.StructType([\n",
    "    T.StructField(\"order_id\", T.IntegerType(), False),\n",
    "    T.StructField(\"customer_id\", T.IntegerType(), True),\n",
    "    T.StructField(\"order_date\", T.TimestampType(), True),\n",
    "])\n",
    "order_items_schema = T.StructType([\n",
    "    T.StructField(\"order_item_id\", T.IntegerType(), False),\n",
    "    T.StructField(\"order_id\", T.IntegerType(), True),\n",
    "    T.StructField(\"product_id\", T.IntegerType(), True),\n",
    "    T.StructField(\"quantity\", T.IntegerType(), True),\n",
    "    T.StructField(\"unit_price\", T.DoubleType(), True),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "679b5072-c33d-4e6e-95d5-23a5b8468fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/Justine/Data_engineering/lab2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Affiche le chemin absolu du répertoire courant\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b95828c",
   "metadata": {},
   "source": [
    "## 1. Ingest operational tables (from CSV exports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "582d2fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customers 24\n",
      "brands 8\n",
      "categories 9\n",
      "products 60\n",
      "orders 220\n",
      "order_items 638\n"
     ]
    }
   ],
   "source": [
    "customers = spark.read.schema(customers_schema).option(\"header\",\"true\").csv(base+\"lab2_customers.csv\")\n",
    "brands = spark.read.schema(brands_schema).option(\"header\",\"true\").csv(base+\"lab2_brands.csv\")\n",
    "categories = spark.read.schema(categories_schema).option(\"header\",\"true\").csv(base+\"lab2_categories.csv\")\n",
    "products = spark.read.schema(products_schema).option(\"header\",\"true\").csv(base+\"lab2_products.csv\")\n",
    "orders = spark.read.schema(orders_schema).option(\"header\",\"true\").csv(base+\"lab2_orders.csv\")\n",
    "order_items = spark.read.schema(order_items_schema).option(\"header\",\"true\").csv(base+\"lab2_order_items.csv\")\n",
    "\n",
    "for name, df in [(\"customers\",customers),(\"brands\",brands),(\"categories\",categories),(\"products\",products),(\"orders\",orders),(\"order_items\",order_items)]:\n",
    "    print(name, df.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7da8f525-3226-4d4f-8c9a-1a84fef233c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def export_spark_metrics_lab2(spark, run_id, task, note=\"\", output_csv=\"lab2_metrics_log.csv\"):\n",
    "    \"\"\"\n",
    "    Exporte les métriques Spark (stages) pour Lab2 dans un CSV.\n",
    "    - spark: SparkSession active\n",
    "    - run_id: identifiant du run (ex: \"r1\")\n",
    "    - task: nom de la tâche (ex: \"ingest_plan\")\n",
    "    - note: commentaire ou variante testée\n",
    "    - output_csv: nom du fichier de sortie\n",
    "    \"\"\"\n",
    "    # Trigger computation pour s'assurer que Spark a fini le job\n",
    "    try:\n",
    "        spark.sparkContext.statusTracker().getJobIdsForGroup(None)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    ui_url = spark.sparkContext.uiWebUrl\n",
    "    app_id = spark.sparkContext.applicationId\n",
    "    \n",
    "    if not ui_url:\n",
    "        print(\"Pas d'UI Spark détectée (aucun job en cours ?)\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        stages_url = f\"{ui_url}/api/v1/applications/{app_id}/stages\"\n",
    "        stages = requests.get(stages_url).json()\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur API Spark: {e}\")\n",
    "        return\n",
    "    \n",
    "    data = []\n",
    "    for s in stages:\n",
    "        stage_id = s.get(\"stageId\")\n",
    "        job_ids = s.get(\"jobIds\", [])\n",
    "        \n",
    "        data.append({\n",
    "            \"run_id\": task,  # on met le nom de la task ici\n",
    "            \"task\": task,\n",
    "            \"note\": note,\n",
    "            \"files_read\": s.get(\"inputRecords\", 0),\n",
    "            \"input_size_bytes\": s.get(\"inputBytes\", 0),\n",
    "            \"shuffle_read_bytes\": s.get(\"shuffleReadBytes\", 0),\n",
    "            \"shuffle_write_bytes\": s.get(\"shuffleWriteBytes\", 0),\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    if df.empty:\n",
    "        print(f\"Aucune métrique trouvée pour {task} (job peut-être pas encore exécuté)\")\n",
    "        return\n",
    "    \n",
    "    # Écrire ou ajouter au CSV\n",
    "    if os.path.exists(output_csv):\n",
    "        df.to_csv(output_csv, mode=\"a\", header=False, index=False)\n",
    "    else:\n",
    "        df.to_csv(output_csv, index=False)\n",
    "    \n",
    "    print(f\"{len(df)} lignes exportées vers {output_csv} pour la tâche {task}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177b7d89",
   "metadata": {},
   "source": [
    "### Evidence: ingestion plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d06c024c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (11)\n",
      "+- HashAggregate (10)\n",
      "   +- Exchange (9)\n",
      "      +- HashAggregate (8)\n",
      "         +- Project (7)\n",
      "            +- BroadcastHashJoin Inner BuildLeft (6)\n",
      "               :- BroadcastExchange (3)\n",
      "               :  +- Filter (2)\n",
      "               :     +- Scan csv  (1)\n",
      "               +- Filter (5)\n",
      "                  +- Scan csv  (4)\n",
      "\n",
      "\n",
      "(1) Scan csv \n",
      "Output [1]: [order_id#403]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/mnt/c/Users/Justine/Data_engineering/lab2/data/lab2_orders.csv]\n",
      "PushedFilters: [IsNotNull(order_id)]\n",
      "ReadSchema: struct<order_id:int>\n",
      "\n",
      "(2) Filter\n",
      "Input [1]: [order_id#403]\n",
      "Condition : isnotnull(order_id#403)\n",
      "\n",
      "(3) BroadcastExchange\n",
      "Input [1]: [order_id#403]\n",
      "Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=3615]\n",
      "\n",
      "(4) Scan csv \n",
      "Output [1]: [order_id#407]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/mnt/c/Users/Justine/Data_engineering/lab2/data/lab2_order_items.csv]\n",
      "PushedFilters: [IsNotNull(order_id)]\n",
      "ReadSchema: struct<order_id:int>\n",
      "\n",
      "(5) Filter\n",
      "Input [1]: [order_id#407]\n",
      "Condition : isnotnull(order_id#407)\n",
      "\n",
      "(6) BroadcastHashJoin\n",
      "Left keys [1]: [order_id#403]\n",
      "Right keys [1]: [order_id#407]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(7) Project\n",
      "Output [1]: [order_id#403]\n",
      "Input [2]: [order_id#403, order_id#407]\n",
      "\n",
      "(8) HashAggregate\n",
      "Input [1]: [order_id#403]\n",
      "Keys [1]: [order_id#403]\n",
      "Functions: []\n",
      "Aggregate Attributes: []\n",
      "Results [1]: [order_id#403]\n",
      "\n",
      "(9) Exchange\n",
      "Input [1]: [order_id#403]\n",
      "Arguments: hashpartitioning(order_id#403, 200), ENSURE_REQUIREMENTS, [plan_id=3620]\n",
      "\n",
      "(10) HashAggregate\n",
      "Input [1]: [order_id#403]\n",
      "Keys [1]: [order_id#403]\n",
      "Functions: []\n",
      "Aggregate Attributes: []\n",
      "Results [1]: [order_id#403]\n",
      "\n",
      "(11) AdaptiveSparkPlan\n",
      "Output [1]: [order_id#403]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n",
      "Saved proof/plan_ingest.txt\n"
     ]
    }
   ],
   "source": [
    "ingest = orders.join(order_items, \"order_id\").select(\"order_id\").distinct()\n",
    "ingest.explain(\"formatted\")\n",
    "export_spark_metrics_lab2(spark, run_id=\"r1\", task=\"ingest_plan\", note=\"ingest operational tables\")\n",
    "\n",
    "from datetime import datetime as _dt\n",
    "import pathlib\n",
    "pathlib.Path(\"proof\").mkdir(exist_ok=True)\n",
    "with open(\"proof/plan_ingest.txt\",\"w\") as f:\n",
    "    f.write(str(_dt.now())+\"\\n\")\n",
    "    f.write(ingest._jdf.queryExecution().executedPlan().toString())\n",
    "print(\"Saved proof/plan_ingest.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df762ce",
   "metadata": {},
   "source": [
    "## 2. Surrogate key function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8d27146a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sk(cols):\n",
    "    # stable 64-bit positive surrogate key from natural keys\n",
    "    return F.abs(F.xxhash64(*[F.col(c) for c in cols]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c29bb3",
   "metadata": {},
   "source": [
    "## 3. Build dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9668fa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_customer = customers.select(\n",
    "    sk([\"customer_id\"]).alias(\"customer_sk\"),\n",
    "    \"customer_id\",\"name\",\"email\",\"created_at\"\n",
    ")\n",
    "\n",
    "dim_brand = brands.select(\n",
    "    sk([\"brand_id\"]).alias(\"brand_sk\"),\n",
    "    \"brand_id\",\"brand_name\"\n",
    ")\n",
    "\n",
    "dim_category = categories.select(\n",
    "    sk([\"category_id\"]).alias(\"category_sk\"),\n",
    "    \"category_id\",\"category_name\"\n",
    ")\n",
    "\n",
    "dim_product = products.select(\n",
    "    sk([\"product_id\"]).alias(\"product_sk\"),\n",
    "    \"product_id\",\"product_name\",\n",
    "    sk([\"brand_id\"]).alias(\"brand_sk\"),\n",
    "    sk([\"category_id\"]).alias(\"category_sk\"),\n",
    "    \"price\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e369d3e",
   "metadata": {},
   "source": [
    "## 4. Build date dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "edf4a037",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window as W\n",
    "dates = orders.select(F.to_date(\"order_date\").alias(\"date\")).distinct()\n",
    "dim_date = dates.select(\n",
    "    sk([\"date\"]).alias(\"date_sk\"),\n",
    "    F.col(\"date\"),\n",
    "    F.year(\"date\").alias(\"year\"),\n",
    "    F.month(\"date\").alias(\"month\"),\n",
    "    F.dayofmonth(\"date\").alias(\"day\"),\n",
    "    F.date_format(\"date\",\"E\").alias(\"dow\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eef97e7",
   "metadata": {},
   "source": [
    "## 5. Build fact_sales with broadcast joins where appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "282e07e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (19)\n",
      "+- Project (18)\n",
      "   +- Project (17)\n",
      "      +- BroadcastHashJoin Inner BuildRight (16)\n",
      "         :- Project (12)\n",
      "         :  +- BroadcastHashJoin Inner BuildRight (11)\n",
      "         :     :- Project (7)\n",
      "         :     :  +- BroadcastHashJoin Inner BuildRight (6)\n",
      "         :     :     :- Filter (2)\n",
      "         :     :     :  +- Scan csv  (1)\n",
      "         :     :     +- BroadcastExchange (5)\n",
      "         :     :        +- Filter (4)\n",
      "         :     :           +- Scan csv  (3)\n",
      "         :     +- BroadcastExchange (10)\n",
      "         :        +- Filter (9)\n",
      "         :           +- Scan csv  (8)\n",
      "         +- BroadcastExchange (15)\n",
      "            +- Filter (14)\n",
      "               +- Scan csv  (13)\n",
      "\n",
      "\n",
      "(1) Scan csv \n",
      "Output [4]: [order_id#407, product_id#408, quantity#409, unit_price#410]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/mnt/c/Users/Justine/Data_engineering/lab2/data/lab2_order_items.csv]\n",
      "PushedFilters: [IsNotNull(product_id), IsNotNull(order_id)]\n",
      "ReadSchema: struct<order_id:int,product_id:int,quantity:int,unit_price:double>\n",
      "\n",
      "(2) Filter\n",
      "Input [4]: [order_id#407, product_id#408, quantity#409, unit_price#410]\n",
      "Condition : (isnotnull(product_id#408) AND isnotnull(order_id#407))\n",
      "\n",
      "(3) Scan csv \n",
      "Output [1]: [product_id#398]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/mnt/c/Users/Justine/Data_engineering/lab2/data/lab2_products.csv]\n",
      "PushedFilters: [IsNotNull(product_id)]\n",
      "ReadSchema: struct<product_id:int>\n",
      "\n",
      "(4) Filter\n",
      "Input [1]: [product_id#398]\n",
      "Condition : isnotnull(product_id#398)\n",
      "\n",
      "(5) BroadcastExchange\n",
      "Input [1]: [product_id#398]\n",
      "Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=3689]\n",
      "\n",
      "(6) BroadcastHashJoin\n",
      "Left keys [1]: [product_id#408]\n",
      "Right keys [1]: [product_id#398]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(7) Project\n",
      "Output [4]: [order_id#407, product_id#408, quantity#409, unit_price#410]\n",
      "Input [5]: [order_id#407, product_id#408, quantity#409, unit_price#410, product_id#398]\n",
      "\n",
      "(8) Scan csv \n",
      "Output [3]: [order_id#403, customer_id#404, order_date#405]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/mnt/c/Users/Justine/Data_engineering/lab2/data/lab2_orders.csv]\n",
      "PushedFilters: [IsNotNull(order_id), IsNotNull(customer_id)]\n",
      "ReadSchema: struct<order_id:int,customer_id:int,order_date:timestamp>\n",
      "\n",
      "(9) Filter\n",
      "Input [3]: [order_id#403, customer_id#404, order_date#405]\n",
      "Condition : (isnotnull(order_id#403) AND isnotnull(customer_id#404))\n",
      "\n",
      "(10) BroadcastExchange\n",
      "Input [3]: [order_id#403, customer_id#404, order_date#405]\n",
      "Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=3693]\n",
      "\n",
      "(11) BroadcastHashJoin\n",
      "Left keys [1]: [order_id#407]\n",
      "Right keys [1]: [order_id#403]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(12) Project\n",
      "Output [6]: [order_id#407, product_id#408, quantity#409, unit_price#410, customer_id#404, order_date#405]\n",
      "Input [7]: [order_id#407, product_id#408, quantity#409, unit_price#410, order_id#403, customer_id#404, order_date#405]\n",
      "\n",
      "(13) Scan csv \n",
      "Output [1]: [customer_id#390]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/mnt/c/Users/Justine/Data_engineering/lab2/data/lab2_customers.csv]\n",
      "PushedFilters: [IsNotNull(customer_id)]\n",
      "ReadSchema: struct<customer_id:int>\n",
      "\n",
      "(14) Filter\n",
      "Input [1]: [customer_id#390]\n",
      "Condition : isnotnull(customer_id#390)\n",
      "\n",
      "(15) BroadcastExchange\n",
      "Input [1]: [customer_id#390]\n",
      "Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=3697]\n",
      "\n",
      "(16) BroadcastHashJoin\n",
      "Left keys [1]: [customer_id#404]\n",
      "Right keys [1]: [customer_id#390]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(17) Project\n",
      "Output [6]: [customer_id#404, order_id#407, product_id#408, quantity#409, unit_price#410, cast(order_date#405 as date) AS date#474]\n",
      "Input [7]: [order_id#407, product_id#408, quantity#409, unit_price#410, customer_id#404, order_date#405, customer_id#390]\n",
      "\n",
      "(18) Project\n",
      "Output [9]: [order_id#407, abs(xxhash64(date#474, 42)) AS date_sk#475L, abs(xxhash64(customer_id#404, 42)) AS customer_sk#476L, abs(xxhash64(product_id#408, 42)) AS product_sk#477L, quantity#409, unit_price#410, (cast(quantity#409 as double) * unit_price#410) AS subtotal#480, year(date#474) AS year#481, month(date#474) AS month#482]\n",
      "Input [6]: [customer_id#404, order_id#407, product_id#408, quantity#409, unit_price#410, date#474]\n",
      "\n",
      "(19) AdaptiveSparkPlan\n",
      "Output [9]: [order_id#407, date_sk#475L, customer_sk#476L, product_sk#477L, quantity#409, unit_price#410, subtotal#480, year#481, month#482]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n",
      "Saved proof/plan_fact_join.txt\n"
     ]
    }
   ],
   "source": [
    "oi = order_items.alias(\"oi\")\n",
    "p = products.alias(\"p\")\n",
    "o = orders.alias(\"o\")\n",
    "c = customers.alias(\"c\")\n",
    "\n",
    "# Join with small dimensions using DF copies to compute SKs, then broadcast dims by size heuristic\n",
    "df_fact = (oi\n",
    "    .join(p, F.col(\"oi.product_id\")==F.col(\"p.product_id\"))\n",
    "    .drop(F.col(\"p.product_id\"))\n",
    "    .join(o, \"order_id\")\n",
    "    .join(c, \"customer_id\")\n",
    "    .withColumn(\"date\", F.to_date(\"order_date\"))\n",
    ")\n",
    "\n",
    "# Attach surrogate keys\n",
    "df_fact = (df_fact\n",
    "    .withColumn(\"date_sk\", sk([\"date\"]))\n",
    "    .withColumn(\"customer_sk\", sk([\"customer_id\"]))\n",
    "    .withColumn(\"product_sk\", sk([\"product_id\"]))\n",
    "    .withColumn(\"quantity\", F.col(\"quantity\").cast(\"int\"))\n",
    "    .withColumn(\"unit_price\", F.col(\"unit_price\").cast(\"double\"))\n",
    "    .withColumn(\"subtotal\", F.col(\"quantity\")*F.col(\"unit_price\"))\n",
    "    .withColumn(\"year\", F.year(\"date\"))\n",
    "    .withColumn(\"month\", F.month(\"date\"))\n",
    "    .select(\"order_id\",\"date_sk\",\"customer_sk\",\"product_sk\",\"quantity\",\"unit_price\",\"subtotal\",\"year\",\"month\")\n",
    ")\n",
    "\n",
    "df_fact.explain(\"formatted\")\n",
    "export_spark_metrics_lab2(spark, run_id=\"r1\", task=\"fact_join\", note=\"build fact_sales with SKs\")\n",
    "with open(\"proof/plan_fact_join.txt\",\"w\") as f:\n",
    "    from datetime import datetime as _dt\n",
    "    f.write(str(_dt.now())+\"\\n\")\n",
    "    f.write(df_fact._jdf.queryExecution().executedPlan().toString())\n",
    "print(\"Saved proof/plan_fact_join.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684d3d81",
   "metadata": {},
   "source": [
    "## 6. Write Parquet outputs (partitioned by year, month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d5b36f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet written under outputs\n"
     ]
    }
   ],
   "source": [
    "base_out = \"outputs\"\n",
    "(dim_customer.write.mode(\"overwrite\").parquet(f\"{base_out}/dim_customer\"))\n",
    "(dim_brand.write.mode(\"overwrite\").parquet(f\"{base_out}/dim_brand\"))\n",
    "(dim_category.write.mode(\"overwrite\").parquet(f\"{base_out}/dim_category\"))\n",
    "(dim_product.write.mode(\"overwrite\").parquet(f\"{base_out}/dim_product\"))\n",
    "(dim_date.write.mode(\"overwrite\").parquet(f\"{base_out}/dim_date\"))\n",
    "(df_fact.write.mode(\"overwrite\").partitionBy(\"year\",\"month\").parquet(f\"{base_out}/fact_sales\"))\n",
    "print(\"Parquet written under outputs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b74e37",
   "metadata": {},
   "source": [
    "## 7. Plan comparison: projection and layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7b8e388e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (16)\n",
      "+- HashAggregate (15)\n",
      "   +- Exchange (14)\n",
      "      +- HashAggregate (13)\n",
      "         +- Project (12)\n",
      "            +- BroadcastHashJoin Inner BuildRight (11)\n",
      "               :- Project (7)\n",
      "               :  +- BroadcastHashJoin Inner BuildLeft (6)\n",
      "               :     :- BroadcastExchange (3)\n",
      "               :     :  +- Filter (2)\n",
      "               :     :     +- Scan csv  (1)\n",
      "               :     +- Filter (5)\n",
      "               :        +- Scan csv  (4)\n",
      "               +- BroadcastExchange (10)\n",
      "                  +- Filter (9)\n",
      "                     +- Scan csv  (8)\n",
      "\n",
      "\n",
      "(1) Scan csv \n",
      "Output [2]: [order_id#403, order_date#405]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/mnt/c/Users/Justine/Data_engineering/lab2/data/lab2_orders.csv]\n",
      "PushedFilters: [IsNotNull(order_id)]\n",
      "ReadSchema: struct<order_id:int,order_date:timestamp>\n",
      "\n",
      "(2) Filter\n",
      "Input [2]: [order_id#403, order_date#405]\n",
      "Condition : isnotnull(order_id#403)\n",
      "\n",
      "(3) BroadcastExchange\n",
      "Input [2]: [order_id#403, order_date#405]\n",
      "Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=4214]\n",
      "\n",
      "(4) Scan csv \n",
      "Output [3]: [order_id#407, product_id#408, quantity#409]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/mnt/c/Users/Justine/Data_engineering/lab2/data/lab2_order_items.csv]\n",
      "PushedFilters: [IsNotNull(order_id), IsNotNull(product_id)]\n",
      "ReadSchema: struct<order_id:int,product_id:int,quantity:int>\n",
      "\n",
      "(5) Filter\n",
      "Input [3]: [order_id#407, product_id#408, quantity#409]\n",
      "Condition : (isnotnull(order_id#407) AND isnotnull(product_id#408))\n",
      "\n",
      "(6) BroadcastHashJoin\n",
      "Left keys [1]: [order_id#403]\n",
      "Right keys [1]: [order_id#407]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(7) Project\n",
      "Output [3]: [order_date#405, product_id#408, quantity#409]\n",
      "Input [5]: [order_id#403, order_date#405, order_id#407, product_id#408, quantity#409]\n",
      "\n",
      "(8) Scan csv \n",
      "Output [2]: [product_id#398, price#402]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/mnt/c/Users/Justine/Data_engineering/lab2/data/lab2_products.csv]\n",
      "PushedFilters: [IsNotNull(product_id)]\n",
      "ReadSchema: struct<product_id:int,price:double>\n",
      "\n",
      "(9) Filter\n",
      "Input [2]: [product_id#398, price#402]\n",
      "Condition : isnotnull(product_id#398)\n",
      "\n",
      "(10) BroadcastExchange\n",
      "Input [2]: [product_id#398, price#402]\n",
      "Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=4218]\n",
      "\n",
      "(11) BroadcastHashJoin\n",
      "Left keys [1]: [product_id#408]\n",
      "Right keys [1]: [product_id#398]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(12) Project\n",
      "Output [3]: [quantity#409, price#402, cast(order_date#405 as date) AS _groupingexpression#520]\n",
      "Input [5]: [order_date#405, product_id#408, quantity#409, product_id#398, price#402]\n",
      "\n",
      "(13) HashAggregate\n",
      "Input [3]: [quantity#409, price#402, _groupingexpression#520]\n",
      "Keys [1]: [_groupingexpression#520]\n",
      "Functions [1]: [partial_sum((cast(quantity#409 as double) * price#402))]\n",
      "Aggregate Attributes [1]: [sum#521]\n",
      "Results [2]: [_groupingexpression#520, sum#522]\n",
      "\n",
      "(14) Exchange\n",
      "Input [2]: [_groupingexpression#520, sum#522]\n",
      "Arguments: hashpartitioning(_groupingexpression#520, 200), ENSURE_REQUIREMENTS, [plan_id=4223]\n",
      "\n",
      "(15) HashAggregate\n",
      "Input [2]: [_groupingexpression#520, sum#522]\n",
      "Keys [1]: [_groupingexpression#520]\n",
      "Functions [1]: [sum((cast(quantity#409 as double) * price#402))]\n",
      "Aggregate Attributes [1]: [sum((cast(quantity#409 as double) * price#402))#519]\n",
      "Results [2]: [_groupingexpression#520 AS d#506, sum((cast(quantity#409 as double) * price#402))#519 AS gmv#507]\n",
      "\n",
      "(16) AdaptiveSparkPlan\n",
      "Output [2]: [d#506, gmv#507]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (16)\n",
      "+- HashAggregate (15)\n",
      "   +- Exchange (14)\n",
      "      +- HashAggregate (13)\n",
      "         +- Project (12)\n",
      "            +- BroadcastHashJoin Inner BuildRight (11)\n",
      "               :- Project (7)\n",
      "               :  +- BroadcastHashJoin Inner BuildLeft (6)\n",
      "               :     :- BroadcastExchange (3)\n",
      "               :     :  +- Filter (2)\n",
      "               :     :     +- Scan csv  (1)\n",
      "               :     +- Filter (5)\n",
      "               :        +- Scan csv  (4)\n",
      "               +- BroadcastExchange (10)\n",
      "                  +- Filter (9)\n",
      "                     +- Scan csv  (8)\n",
      "\n",
      "\n",
      "(1) Scan csv \n",
      "Output [2]: [order_id#403, order_date#405]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/mnt/c/Users/Justine/Data_engineering/lab2/data/lab2_orders.csv]\n",
      "PushedFilters: [IsNotNull(order_id)]\n",
      "ReadSchema: struct<order_id:int,order_date:timestamp>\n",
      "\n",
      "(2) Filter\n",
      "Input [2]: [order_id#403, order_date#405]\n",
      "Condition : isnotnull(order_id#403)\n",
      "\n",
      "(3) BroadcastExchange\n",
      "Input [2]: [order_id#403, order_date#405]\n",
      "Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=4589]\n",
      "\n",
      "(4) Scan csv \n",
      "Output [3]: [order_id#407, product_id#408, quantity#409]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/mnt/c/Users/Justine/Data_engineering/lab2/data/lab2_order_items.csv]\n",
      "PushedFilters: [IsNotNull(order_id), IsNotNull(product_id)]\n",
      "ReadSchema: struct<order_id:int,product_id:int,quantity:int>\n",
      "\n",
      "(5) Filter\n",
      "Input [3]: [order_id#407, product_id#408, quantity#409]\n",
      "Condition : (isnotnull(order_id#407) AND isnotnull(product_id#408))\n",
      "\n",
      "(6) BroadcastHashJoin\n",
      "Left keys [1]: [order_id#403]\n",
      "Right keys [1]: [order_id#407]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(7) Project\n",
      "Output [3]: [order_date#405, product_id#408, quantity#409]\n",
      "Input [5]: [order_id#403, order_date#405, order_id#407, product_id#408, quantity#409]\n",
      "\n",
      "(8) Scan csv \n",
      "Output [2]: [product_id#398, price#402]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/mnt/c/Users/Justine/Data_engineering/lab2/data/lab2_products.csv]\n",
      "PushedFilters: [IsNotNull(product_id)]\n",
      "ReadSchema: struct<product_id:int,price:double>\n",
      "\n",
      "(9) Filter\n",
      "Input [2]: [product_id#398, price#402]\n",
      "Condition : isnotnull(product_id#398)\n",
      "\n",
      "(10) BroadcastExchange\n",
      "Input [2]: [product_id#398, price#402]\n",
      "Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=4593]\n",
      "\n",
      "(11) BroadcastHashJoin\n",
      "Left keys [1]: [product_id#408]\n",
      "Right keys [1]: [product_id#398]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(12) Project\n",
      "Output [3]: [quantity#409, price#402, cast(order_date#405 as date) AS _groupingexpression#543]\n",
      "Input [5]: [order_date#405, product_id#408, quantity#409, product_id#398, price#402]\n",
      "\n",
      "(13) HashAggregate\n",
      "Input [3]: [quantity#409, price#402, _groupingexpression#543]\n",
      "Keys [1]: [_groupingexpression#543]\n",
      "Functions [1]: [partial_sum((cast(quantity#409 as double) * price#402))]\n",
      "Aggregate Attributes [1]: [sum#544]\n",
      "Results [2]: [_groupingexpression#543, sum#545]\n",
      "\n",
      "(14) Exchange\n",
      "Input [2]: [_groupingexpression#543, sum#545]\n",
      "Arguments: hashpartitioning(_groupingexpression#543, 200), ENSURE_REQUIREMENTS, [plan_id=4598]\n",
      "\n",
      "(15) HashAggregate\n",
      "Input [2]: [_groupingexpression#543, sum#545]\n",
      "Keys [1]: [_groupingexpression#543]\n",
      "Functions [1]: [sum((cast(quantity#409 as double) * price#402))]\n",
      "Aggregate Attributes [1]: [sum((cast(quantity#409 as double) * price#402))#542]\n",
      "Results [2]: [_groupingexpression#543 AS d#535, sum((cast(quantity#409 as double) * price#402))#542 AS gmv#536]\n",
      "\n",
      "(16) AdaptiveSparkPlan\n",
      "Output [2]: [d#535, gmv#536]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n",
      "Record Spark UI metrics for both runs in lab2_metrics_log.csv\n"
     ]
    }
   ],
   "source": [
    "# Case A: join and then project\n",
    "a = (orders.join(order_items, \"order_id\")\n",
    "            .join(products, \"product_id\")\n",
    "            .groupBy(F.to_date(\"order_date\").alias(\"d\"))\n",
    "            .agg(F.sum(F.col(\"quantity\")*F.col(\"price\")).alias(\"gmv\")))\n",
    "a.explain(\"formatted\")\n",
    "_ = a.count()\n",
    "export_spark_metrics_lab2(spark, run_id=\"r1\", task=\"caseA_join_then_project\", note=\"join then aggregate\")\n",
    "\n",
    "# Case B: project early\n",
    "b = (orders.select(\"order_id\",\"order_date\")\n",
    "            .join(order_items.select(\"order_id\",\"product_id\",\"quantity\"), \"order_id\")\n",
    "            .join(products.select(\"product_id\",\"price\"), \"product_id\")\n",
    "            .groupBy(F.to_date(\"order_date\").alias(\"d\"))\n",
    "            .agg(F.sum(F.col(\"quantity\")*F.col(\"price\")).alias(\"gmv\")))\n",
    "b.explain(\"formatted\")\n",
    "_ = b.count()\n",
    "export_spark_metrics_lab2(spark, run_id=\"r1\", task=\"caseB_project_then_join\", note=\"project early then join\")\n",
    "\n",
    "print(\"Record Spark UI metrics for both runs in lab2_metrics_log.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd1a184",
   "metadata": {},
   "source": [
    "## 8. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f8046963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session stopped.\n"
     ]
    }
   ],
   "source": [
    "spark.stop()\n",
    "print(\"Spark session stopped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31ad493-a9f8-46fa-bd41-7bca4da839d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (de1-env)",
   "language": "python",
   "name": "de1-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
