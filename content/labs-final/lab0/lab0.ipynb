{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b34599e-ca38-47fc-9208-96d5956d4615",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/10/15 10:49:47 WARN Utils: Your hostname, OrdideJustine, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/10/15 10:49:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/15 10:49:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark: 4.0.1\n",
      "PySpark: 4.0.1\n"
     ]
    }
   ],
   "source": [
    "import findspark, pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "findspark.init()\n",
    "spark = SparkSession.builder.appName(\"de1-check\").getOrCreate()\n",
    "print(\"Spark:\", spark.version)\n",
    "print(\"PySpark:\", pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "122a3dcc-12a9-4cab-a60a-a315403461c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote Data_engineering/data/sample_sales.csv, bytes: 154\n",
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      "\n",
      "+-------+----------+-----+-------------------+\n",
      "|user_id|product_id|price|                 ts|\n",
      "+-------+----------+-----+-------------------+\n",
      "|      1|       101|  9.9|2025-09-01 09:00:00|\n",
      "|      1|       102| 19.0|2025-09-01 09:02:00|\n",
      "|      2|       101|  9.9|2025-09-02 10:00:00|\n",
      "|      3|       103|  5.5|2025-09-03 11:30:00|\n",
      "+-------+----------+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, csv, pathlib\n",
    "pathlib.Path(\"Data_engineering/data\").mkdir(exist_ok=True)\n",
    "rows = [\n",
    "    {\"user_id\":1,\"product_id\":101,\"price\":9.9,\"ts\":\"2025-09-01T09:00:00\"},\n",
    "    {\"user_id\":1,\"product_id\":102,\"price\":19.0,\"ts\":\"2025-09-01T09:02:00\"},\n",
    "    {\"user_id\":2,\"product_id\":101,\"price\":9.9,\"ts\":\"2025-09-02T10:00:00\"},\n",
    "    {\"user_id\":3,\"product_id\":103,\"price\":5.5,\"ts\":\"2025-09-03T11:30:00\"},\n",
    "]\n",
    "with open(\"Data_engineering/data/sample_sales.csv\",\"w\",newline=\"\") as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"user_id\",\"product_id\",\"price\",\"ts\"])\n",
    "    w.writeheader(); w.writerows(rows)\n",
    "print(\"Wrote Data_engineering/data/sample_sales.csv, bytes:\", os.path.getsize(\"Data_engineering/data/sample_sales.csv\"))\n",
    "df = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"Data_engineering/data/sample_sales.csv\")\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c0bccb7-5fbb-46f5-baca-842d125c63ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      "\n",
      "+-------+----------+-----+-------------------+\n",
      "|user_id|product_id|price|                 ts|\n",
      "+-------+----------+-----+-------------------+\n",
      "|      1|       101|  9.9|2025-09-01 09:00:00|\n",
      "|      1|       102| 19.0|2025-09-01 09:02:00|\n",
      "|      2|       101|  9.9|2025-09-02 10:00:00|\n",
      "|      3|       103|  5.5|2025-09-03 11:30:00|\n",
      "+-------+----------+-----+-------------------+\n",
      "\n",
      "== Physical Plan ==\n",
      "Scan csv  (1)\n",
      "\n",
      "\n",
      "(1) Scan csv \n",
      "Output [4]: [user_id#251, product_id#252, price#253, ts#254]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/mnt/c/Users/Justine/Data_engineering/data/sample_sales.csv]\n",
      "ReadSchema: struct<user_id:int,product_id:int,price:double,ts:timestamp>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"Data_engineering/data/sample_sales.csv\")\n",
    "df.printSchema()\n",
    "df.show()\n",
    "df.explain(\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed6586f5-6952-4952-97be-0586fd174f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-----+\n",
      "|user_id|  n|total|\n",
      "+-------+---+-----+\n",
      "|      1|  2| 28.9|\n",
      "|      3|  1|  5.5|\n",
      "|      2|  1|  9.9|\n",
      "+-------+---+-----+\n",
      "\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[user_id#251], functions=[count(1), sum(price#253)], output=[user_id#251, n#351L, total#352])\n",
      "   +- Exchange hashpartitioning(user_id#251, 200), ENSURE_REQUIREMENTS, [plan_id=441]\n",
      "      +- HashAggregate(keys=[user_id#251], functions=[partial_count(1), partial_sum(price#253)], output=[user_id#251, count#364L, sum#365])\n",
      "         +- FileScan csv [user_id#251,price#253] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/mnt/c/Users/Justine/Data_engineering/data/sample_sales.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<user_id:int,price:double>\n",
      "\n",
      "\n",
      "=== explain formatted ===\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (5)\n",
      "+- HashAggregate (4)\n",
      "   +- Exchange (3)\n",
      "      +- HashAggregate (2)\n",
      "         +- Scan csv  (1)\n",
      "\n",
      "\n",
      "(1) Scan csv \n",
      "Output [2]: [user_id#251, price#253]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/mnt/c/Users/Justine/Data_engineering/data/sample_sales.csv]\n",
      "ReadSchema: struct<user_id:int,price:double>\n",
      "\n",
      "(2) HashAggregate\n",
      "Input [2]: [user_id#251, price#253]\n",
      "Keys [1]: [user_id#251]\n",
      "Functions [2]: [partial_count(1), partial_sum(price#253)]\n",
      "Aggregate Attributes [2]: [count#362L, sum#363]\n",
      "Results [3]: [user_id#251, count#364L, sum#365]\n",
      "\n",
      "(3) Exchange\n",
      "Input [3]: [user_id#251, count#364L, sum#365]\n",
      "Arguments: hashpartitioning(user_id#251, 200), ENSURE_REQUIREMENTS, [plan_id=441]\n",
      "\n",
      "(4) HashAggregate\n",
      "Input [3]: [user_id#251, count#364L, sum#365]\n",
      "Keys [1]: [user_id#251]\n",
      "Functions [2]: [count(1), sum(price#253)]\n",
      "Aggregate Attributes [2]: [count(1)#357L, sum(price#253)#358]\n",
      "Results [3]: [user_id#251, count(1)#357L AS n#351L, sum(price#253)#358 AS total#352]\n",
      "\n",
      "(5) AdaptiveSparkPlan\n",
      "Output [3]: [user_id#251, n#351L, total#352]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n",
      "Saved Data_engineering/proof/plan_formatted.txt\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "agg = df.groupBy(\"user_id\").agg(F.count(\"*\").alias(\"n\"), F.sum(\"price\").alias(\"total\"))\n",
    "agg.show()\n",
    "plan = agg._jdf.queryExecution().executedPlan().toString()\n",
    "print(plan)\n",
    "print(\"\\n=== explain formatted ===\")\n",
    "agg.explain(\"formatted\")\n",
    "\n",
    "# Save evidence\n",
    "import pathlib, datetime\n",
    "pathlib.Path(\"Data_engineering/proof\").mkdir(exist_ok=True)\n",
    "with open(\"Data_engineering/proof/plan_formatted.txt\",\"w\") as f:\n",
    "    f.write(str(datetime.datetime.now()) + \"\\n\\n\")\n",
    "    f.write(str(plan))\n",
    "print(\"Saved Data_engineering/proof/plan_formatted.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fba02481-dca3-4af7-870f-158b0a48561e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session stopped.\n"
     ]
    }
   ],
   "source": [
    "spark.stop()\n",
    "print(\"Spark session stopped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a4a903-f9a3-49e3-83c1-51feaa26c7fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
